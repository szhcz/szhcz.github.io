[{"title":"IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models","date":"2024-10-16T16:00:00.000Z","url":"/posts/IP-Adapter-Text-Compatible-Image-Prompt-Adapter-for-Text-to-Image-Diffusion-Models/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1 研究背景、动机、主要贡献1.1 研究背景近年来，大型文本生成图像扩散模型（如GLIDE、DALL-E 2、Stable Diffusion等）取得了显著进展，能够根据文本提示生成高保真图像。然而，生成理想的图像通常需要复杂的提示词工程，而且文本在表达复杂场景和概念时不够直观。因此，提出图像提示作为替代方案，以增强生成能力。 1.2 存在问题(动机) 微调模型消除原始文本生成能力，且需大量计算资源。 微调后的模型难以复用，无法在基于相同基础模型的自定义模型中直接应用图像提示功能。 与现有的结构控制工具（如ControlNet）不兼容，限制了灵活性。 替换文本编码器的方法虽然简单，但无法同时支持图像提示和文本提示，且生成图像的质量和泛化能力较差。 1.3 主要贡献 提出了IP-Adapter，一种轻量级适配器，采用解耦的交叉注意力机制，将文本和图像特征分别处理，保留原有的文本生成功能。 IP-Adapter仅需22M参数，能够实现与完全微调的图像提示模型相当甚至更优的生成效果。 IP-Adapter可复用，能够泛化至基于同一基础模型微调的其他自定义模型，并与结构控制工具兼容，实现灵活的图像生成任务。 支持文本提示和图像提示的多模态图像生成。 2 论文提出的新方法3.1 PrelimiariesStable Diffusion 3.2 Image Prompt Adapter 现有的方法难以达到那些为图像提示专门精调或从头训练的模型的性能，原因： 现有方法通常将图像特征与文本特征简单地拼接，然后直接输入到冻结的交叉注意力层。图像特征无法有效嵌入到预训练模型中。 这种拼接方式无法让扩散模型充分捕捉图像提示中的细粒度特征，导致生成图像的质量和细节欠佳。 本文提出了一种解耦的交叉注意策略，其中图像特征由新添加的交叉注意层嵌入。 IP-Adapter设计： 图像编码器：使用预训练的CLIP图像编码器从图像提示中提取图像特征。 CLIP是一种多模态模型，通过对比学习在包含大量图像-文本对的数据集上进行训练。它能够生成与图像描述高度匹配的全局图像嵌入，并代表图像的内容和风格。 在训练阶段，CLIP编码器是冻结的，其参数不再更新。 为了更有效地使用全局图像嵌入，论文引入了一个小型可训练的投影网络，将嵌入分解为N个特征序列（本研究中N=4），并且这些特征的维度与预训练扩散模型中的文本特征维度相同。 投影网络包括一个线性层和层归一化，用于处理图像嵌入。 具有解耦交叉注意力的自适应模块：为每一个原有的交叉注意力层添加新的交叉注意力层来处理图像特征。 原始SD： 为原始UNet模型中的每个交叉注意层添加一个新的交叉注意层以插入图像特征。 为了加速收敛， 和 由 和 初始化 然后，我们简单地将图像交叉注意力的输出添加到文本交叉注意力的输出上 训练和推理 在训练过程中，模型只优化IP-Adapter，而保持预训练的扩散模型参数不变。 通过在训练时随机丢弃图像条件，可以在推理时应用无分类器引导。 推理时可以通过调整图像条件的权重因子来控制图像提示的影响。 3 论文方法的实验评估方法与效果3.1 Experimental Setup3.1.1 Training Data为了训练IP-Adapter，构建了一个多模态数据集，包括来自两个开源数据集（LAION-2B和COYO-700M）约1000万个文本-图像对。 3.1.2 Implementation Details 实验基于SD v1.5版本，使用OpenCLIP ViT-H/14作为图像编码器。 SD模型包含16个交叉注意力层，并为每个交叉注意力层添加一个新的图像交叉注意力层。 IP-Adapter的总可训练参数约为2200万，设计较为轻量。 使用HuggingFace diffusers库实现IP-Adapter，并采用DeepSpeed ZeRO-2进行快速训练。 在单台配备8个V100 GPU的机器上进行训练，步数为100万，每个GPU的批量大小为8。 使用AdamW优化器，固定学习率为0.0001，权重衰减为0.01。 在训练过程中，将图像的最短边调整为512，然后中心裁剪为512 × 512分辨率。 为了启用无条件分类器自由指导，在训练阶段以0.05的概率单独丢弃文本和图像，并以0.05的概率同时丢弃文本和图像。 推理阶段采用DDIM采样器，步数为50，指导尺度设置为7.5；当仅使用图像提示时，文本提示为空。 3.2 Comparison with Existing Methods将 IP-Adapter 与其他现有的带有图像提示的生成方法进行比较 从头训练的方法：包括open unCLIP、Kandinsky-2-1和Versatile Diffusion。 从文本到图像模型进行微调：选择 SD Image Variations 和SD unCLIP。 适配器方法：与T2I-Adapter的风格适配器、Uni-ControlNet的全局控制器、ControlNet Shuffle、ControlNet Reference-only和SeeCoder进行比较。 3.2.1 Quantitative Comparison使用COCO2017验证集（包含5000张带有字幕的图像）进行定量评估。为了公平比较，对每个样本生成4张基于图像提示的图像，生成20000张图像。采用下面两个指标 CLIP-I：生成图像与图像提示在CLIP图像嵌入中的相似度。 CLIP-T：生成图像与图像提示字幕的CLIPScore。 3.2.2 Qualitative Comparison从各种类型和风格的图像中随机生成4个样本，并选择每种方法中表现最佳的一个。 3.3 More Results3.3.1 Generalizable to Custom ModelsIP-Adapter可以在训练阶段冻结原始扩散模型，因此可推广到从SD v1.5微调的自定义模型。一旦训练完成，IP-Adapter可以直接应用于社区模型，并能生成与这些模型风格相符的图像。 3.3.2 Structure ControlIP-Adapter完全兼容现有的可控工具，可以生成带有图像提示和附加条件的可控图像。 3.3.3 Image-to-Image and Inpainting可以实现文本引导/图像引导的图像到图像和修复。 3.3.4 Multimodal Prompts对于完全微调的图像提示模型，原始的文本到图像能力几乎丧失。使用IP-Adapter可以生成包括图像提示和文本提示的多模态提示图像。 3.4 Ablation Study3.4.1 Importance of Decoupled Cross-Attention通过比较没有解耦交叉注意力的简单适配器和IP-Adapter，验证了解耦交叉注意力策略的有效性。 3.4.2 Comparison of Fine-grained Features and Global Features利用CLIP图像编码器的全局图像嵌入，可能会丢失参考图像的一些信息。因此，设计了一个基于细粒度特征的IP-Adapter，提取CLIP图像编码器倒数第二层的网格特征，并通过轻量变换模型学习特征。 虽然具有更细粒度特征的IP-Adapter可以生成与图像提示更加一致的图像，但它也可以学习空间结构信息，这可能会减少生成图像的多样性。 4 论文优点、局限性优点： 基于解耦交叉注意力，为预训练的文本到图像扩散模型实现图像提示功能。 IP-Adapter在经过一次训练后，可以直接与基于相同基础模型的自定义模型及现有的结构可控工具集成。 多模态图像生成 局限性： 只能生成内容和风格与参考图像相似的图像。无法像某些现有方法（例如 Textual Inversion 和 DreamBooth ）那样生成与给定图像主题高度一致的图像。 原文链接：IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models"},{"title":"Structured Denoising Diffusion Models in Discrete State-Spaces","date":"2024-10-13T16:00:00.000Z","url":"/posts/Structured-Denoising-Diffusion-Models-in-Discrete-State-Spaces/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1 研究背景、动机、主要贡献 1.1 研究背景 尽管扩散模型在连续数据上表现良好，但在离散数据（如文本或量化后的图像）上的应用仍然有限。现有的离散扩散模型主要集中在文本和图像分割领域，尚未在大规模文本或图像生成任务上展示出竞争力。此外，最近的大多数研究集中在高斯扩散过程上，这种过程应用于连续状态空间（例如，用于处理实数值的图像和波形数据）。而缺乏对离散数据结构或领域知识的利用。 离散扩散模型的背景： 离散扩散模型最初由Sohl-Dickstein等人引入，用于处理二值随机变量的扩散过程。 Hoogeboom等人将这一模型扩展到分类随机变量，使用了均匀转移概率的矩阵来描述这一扩散过程。 Song等人在其补充材料中也推导了这个模型的扩展，但没有做相关实验。 1.3 主要贡献 2 论文提出的新方法 one-hot向量表示 类别分布 (主要利用 此公式化简。且因为马尔可夫过程， ) Kullback-Leibler散度是衡量两个概率分布之间差异的常用方法。在这里，假设 q 和 是两个概率分布，KL散度可以通过将两个分布的差异按条件独立性分开求和，变得易于计算。 依赖于转移矩阵 ，其累积乘积（多个时间步的转移概率）通常可以以封闭形式计算或预先计算。也就是说，可以提前准备这些转移矩阵以加速模型的推理过程。然而，对于大 K 和大 T 来说，这可能是令人望而却步的。 Choice of Markov transition matrices for the forward process 转移矩阵的行必须相加为1，以保证概率质量的守恒。 必须在 t 增大时收敛到已知的平稳分布。 对于大多数现实世界的离散数据（例如图像和文本），在转移矩阵 中添加领域相关的结构是有意义的，这样可以更好地控制前向加噪过程和可学习的反向去噪过程。 均匀转移矩阵 Sohl-Dickstein等人研究了一个简单的2×2转移矩阵用于二元随机变量。Hoogeboom等人将其扩展到类别变量，提出的转移矩阵为 𝟙𝟙 这个转移矩阵是双重随机的，确保了平稳分布是均匀的。这种离散扩散实例被称为D3PM-uniform。 吸收状态 Inspired by BERT和条件掩码语言模型（CMLM），考虑一个具有吸收状态（如[MASK]）的转移矩阵，使得每个标记要么保持不变，要么以概率 t 转移到[MASK]。 这不会强加类别之间的特定关系，类似于均匀扩散。 这种设计允许区分被损坏的标记与原始标记，但平稳分布不是均匀的，所有质量全在[MASK]标记上。 离散化高斯 对于序数数据，建议使用离散化的、截断的高斯分布，模仿连续空间的扩散模型。选择一个归一化使转移矩阵为双重随机的，从而得到均匀的平稳分布。该转换矩阵将以更高的概率在更相似的状态之间转换，并且非常适合图像等量化序数数据。 标记嵌入距离 对于文本数据，没有序数结构，但可能存在有趣的语义关系。使用词嵌入空间的相似性来指导前向过程，构造一个双重随机转移矩阵，在嵌入相似的标记之间更频繁地转移，同时保持均匀的平稳分布。 Noise schedules 对于离散化的高斯扩散，探索在离散化之前线性增加高斯的方差。（ 的线性调度会导致 中累积噪声的非线性增加。） 对于均匀扩散，使用余弦调度，它将转换的累积概率设置为余弦函数。 对于一组通用的转移矩阵 （例如基于令牌嵌入的矩阵），以前提出的调度可能不直接适用。 考虑将 和 之间的互信息（mutual information）线性插值到零，即 在时间 t 时，互信息可以通过一个线性函数来近似，该函数与初始状态 的熵 成正比。随着 t 的增加，互信息逐渐减小，最终为零。 对于吸收状态D3PM的特定情况，这个调度恰好简化为 调度。 Parameterization of the reverse process 使用神经网络 来预测分布 的 logits。我们将其与 结合，并对 的one-hot 表示进行求和，以获得以下参数化形式： (有点像全概率公式) 在处理有序离散数据时，除了直接用神经网络的输出预测 的 logits，还可以使用截断的离散逻辑分布来建模概率。这种方法为反向模型提供了额外的有序归纳偏置，从而提高了图像生成的FID和对数似然得分。 Loss function 包含了一系列的KL散度项，用来衡量模型预测的后验分布和真实后验分布之间的差距。然而，尽管这种优化目标有其理论优势，实际应用中可能会导致优化过于复杂，效果不如预期。 Ho 等人提出了一种简化的损失函数 ，这是一种对负变分下界的重加权。这种方法减少了优化的复杂性，同时在实验中表现出色。通过重加权， 重点放在了某些特定的误差项上，使得模型的训练更为高效。 Nichol 和 Dhariwal 提出了混合损失函数 ，结合了简化损失和变分下界。使用一项来学习预测均值，另一项来学习预测方差。 受这项最近工作的启发，作者引入了一个辅助去噪目标，用于反向过程的 参数化。这个目标旨在在每个时间步预测原始数据 ，鼓励模型更准确地还原数据。这与以往的重加权方法不同，因为它直接对模型的输出 进行监督。作者将这一辅助去噪目标与负变分下界结合，得到一个新的损失函数 KL 重加权是在通过下界项进行间接优化，而新的损失函数则通过直接预测 的概率来进行监督。 原文链接：Structured Denoising Diffusion Models in Discrete State-Spaces"},{"title":"Hexo 公式渲染踩坑","date":"2024-10-11T16:00:00.000Z","url":"/posts/Hexo-formula/","tags":[["Hexo","/tags/Hexo/"]],"categories":[["杂","/categories/%E6%9D%82/"],["Hexo","/categories/%E6%9D%82/Hexo/"]],"content":"renderer选择 解决多行公式（如矩阵）无法渲染问题 我的 node_modules ： hexo-filter-mathjax hexo-renderer-ejs hexo-renderer-kramed hexo-renderer-pandoc hexo-renderer-stylus _config.yml 中 渲染冲突问题 如将 Latex 中下划线渲染为斜体 解决方法： node_modules.js 中 "},{"title":"Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model","date":"2024-10-10T16:00:00.000Z","url":"/posts/Transfusion-Predict-the-Next-Token-and-Diffuse-Images-with-One-Multi-Modal-Model/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"],["Autoregressive Model","/tags/Autoregressive-Model/"],["Transformer","/tags/Transformer/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1 研究背景、动机、主要贡献 主要贡献 Transfusion方法的提出：通过将离散文本标记的预测和连续图像的扩散过程完全整合，Transfusion能够同时处理这两种模态，无信息损失。 多模态训练框架：论文展示了如何在一个统一的模型中，利用不同的目标函数（如文本的下一词预测和图像的扩散）进行训练，从而实现对两种模态的无缝生成。 模型架构的创新：引入了特定的编码和解码层，使得模型能够高效地处理和生成文本与图像，同时压缩图像到较小的表示。 优越的性能表现：实验表明，Transfusion在文本到图像和图像到文本生成任务中，超越了现有的方案，且在计算效率和生成质量上表现优秀。 Related Work 许多现有的多模态模型采用将多个模态特定的架构组合在一起的方式来处理不同的数据类型。这通常包括预训练各个模态的组件，然后将它们连接起来。例如，在图像和视频生成任务中，通常使用大型的预训练文本编码器（如Transformer架构）将输入文本表示为潜在空间中的向量，然后再通过这些表示去引导（condition）扩散模型进行生成。这类方法的特点是利用了多种现成的编码器来提升性能。 许多视觉-语言模型（Vision-Language Models）使用预训练语言模型作为主要结构，并通过投影层与其他模态特定的编码器/解码器相连接。代表性的模型包括： Flamingo（视觉理解） LLaVA（视觉理解） GILL（视觉生成模型） DreamLLM（用于视觉理解和生成） Transfusion模型采用了一个统一的、端到端（end-to-end）的架构，可以同时处理文本和图像的理解与生成。 尽管最近的研究（如Li等人和Gat等人的工作）尝试将扩散模型推广到离散文本生成上，但其在生成质量和模型规模上仍然无法与标准的自回归语言模型相媲美。 2 论文提出的新方法 Transfusion 是一种训练单一统一模型以理解和生成离散和连续模态的方法。主要创新在于证明可以对不同模态使用单独的损失函数——对文本使用语言建模，对图像使用扩散——并在共享数据和参数上进行训练。 Data Representation 离散文本 每个文本字符串被分词成固定词汇表中的离散标记序列，每个标记被表示为一个整数。 连续图像。 每张图像通过 VAE 编码为 latent patches ，其中每个 patch 被表示为一个连续向量；补丁按从左到右、从上到下的顺序排列，形成一系列补丁向量。 对于混合模态示例，在每个图像序列的前后加上特殊的“图像开始”（BOI）和“图像结束”（EOI）标记，然后将其插入到文本序列中；因此，得到一个单一的序列，其中可能包含离散元素（表示文本标记的整数）和连续元素（表示图像补丁的向量）。 Model Architecture 大多数模型参数集中在一个单一的 transformer 上，该 transformer 处理每个序列，无论其模态如何。 transformer 接受高维向量的序列作为输入，并产生类似的向量作为输出。为了将数据转换为该空间，我们使用轻量级的模态特定组件，且不共享参数。 对于文本，这些是嵌入矩阵，将每个输入整数转换为向量空间，将每个输出向量转换为词汇表上的离散分布。 对于图像，我们实验了两种压缩局部窗口（k×k补丁向量）为单个变换器向量的替代方案（反之亦然）： （1）简单的线性层 （2）U-Net的上下采样块。 Transfusion Attention 语言模型通常使用因果掩码，以便在一次前向和反向传播中有效计算整个序列的损失和梯度，而不泄漏未来标记的信息。 图像并不是自然顺序的；图像中的每个部分（像素或图像块）可以在没有特定顺序的情况下相互影响。通常使用无约束（双向）注意力进行建模。 Transfusion 通过对序列中的每个元素应用因果注意力，而在每个图像的元素内应用双向注意力，结合了这两种注意力模式。 这使得每个图像补丁可以关注同一图像内的其他补丁，但只能关注序列中先前出现的文本或其他图像的补丁。 启用图像内部的注意力显著提升了模型性能。 Training Objective 用于文本的预测。模型的目标是最大化文本序列中每个词的概率。损失是针对每个文本 token 计算的。 用于图像的预测。损失是针对每个图像计算的，可能跨越序列中的多个元素（图像补丁）。 生成一个被噪声污染的图像 后。在进行图像块划分之前计算图像级别的扩散损失。 损失函数 。 未来的工作可能会探讨更复杂的损失组合，例如用流匹配（flow matching）替代 diffusion 。 推理 解码算法在两种模式之间切换：LM和扩散。 在LM模式下，算法按照标准做法逐个从预测的分布中采样生成文本令牌（token）。当我们采样到BOI标记时，解码算法切换到扩散模式. 在扩散模式下，我们遵循扩散模型的标准解码过程。具体而言，我们将纯噪声 以n个图像补丁的形式附加到输入序列中，并在T个步骤中去噪。在每一步t，我们获取噪声预测，并利用它生成 ，这将覆盖序列中的 ；即模型始终依赖于噪声图像的最后时间步，并且不能关注之前的时间步。 扩散过程结束后，我们将EOI标记附加到预测的图像上，并切换回LM模式。 3 论文方法的理论分析或实验评估方法与效果 Setup AdamW优化器 随机初始化参数：所有模型参数都被随机初始化，这是一种常见的初始化方式，有助于避免模型陷入不理想的局部最小值。 它结合了Adam优化器的自适应学习率和L2正则化（权重衰减）。具体参数设置为： β1 = 0.9 和 β2 = 0.95：这两个参数控制一阶和二阶动量的指数衰减率，较高的值使得优化器能够更平滑地更新参数。 ε = 1e-8：防止分母出现0值的常数。梯度的平方累积值接近 0 时，除以这些值可能导致数值不稳定。 学习率 学习率预热（warm-up）：初始学习率设为0.0003，在训练的最初4000步中，学习率逐渐上升。有助于防止模型在初期更新时过快收敛到次优解。 余弦调度（cosine scheduler）：学习率在训练的后期逐渐衰减至1.5e-5。平稳降低学习率，避免过度调整模型权重。 权重衰减防止过拟合 梯度裁剪防止梯度爆炸 Controlled Comparison with Chameleon 实验使用了模型大小（N）和token数量（D）作为对比的两个关键参数，以这两者的组合作为计算复杂度（FLOPs）的代理。 尽管Transfusion和Chameleon在文本建模中使用相同的方式，Transfusion依然在文本任务上表现更好。 一种假设是，这源于输出分布中文本和图像标记之间的竞争 或者，扩散可能在图像生成方面更有效并且需要更少的参数，从而允许 Transfusion 模型使用比 Chameleon 更多的容量来建模文本。 Architecture Ablations U-Net层带来的归纳偏置可能是其表现比简单的线性层好的原因。即便在较小的模型中加入这些归纳偏置，也能够提升模型在图像生成和描述任务上的表现。 通过调整扩散噪声的水平，图像描述任务中的性能有所改善。 Transfusion模型不仅能处理文本和图像，还能执行图像编辑任务。 原文链接：Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model"},{"title":"Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction","date":"2024-10-06T16:00:00.000Z","url":"/posts/Visual-Autoregressive-Modeling-Scalable-Image-Generation-via-Next-Scale-Prediction/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"],["Autoregressive Model","/tags/Autoregressive-Model/"],["Transformer","/tags/Transformer/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1 研究背景、动机、主要贡献 1.1 研究背景 标准自回归模型 vs. VAR AR在文本中的应用：按顺序从左到右逐个生成文本标记。 AR在图像中的应用：以类似的方式，从左到右、从上到下逐行生成图像的视觉标记，类似于在文本生成中一行一行地产生像素。 VAR的图像建模：采用多尺度的方式，从粗略到精细的分辨率生成图像。每个尺度的标记是并行生成的。这种生成方式需要多尺度的VQ-VAE来支持。 related work Properties of large autoregressive language models Scaling laws 能够直接从较小的模型预测较大模型的性能 Zero-shot generalization. Visual generation Raster-scan autoregressive models VQGAN和VQVAE Masked-prediction model. Diffusion models 1.2 主要贡献 传统自回归模型逐标记生成图像，而VAR认为人类感知图像是从全局结构到局部细节的层次化过程，因此采用从粗到细、从低分辨率到高分辨率的方式生成图像。每个分辨率内的标记是并行生成的，而非依赖逐行逐列生成。 VAR在性能上首次超越了 DiT ,使得GPT式自回归方法在图像合成中首次超越强扩散模型。在多个维度（包括图像质量、推理速度、数据效率和可扩展性）上表现更优。 开源代码套件 2 论文提出的新方法 2.1 Preliminary: autoregressive modeling via next-token prediction next-token prediction 要将next-token prediction应用于图像生成，必须先将图像标记化（将图像转换为离散的标记）并将2D图像标记序列化为1D序列。 将特征图 转换为离散标记 . 量化过程 会将每个特征向量 映射到欧几里德意义上最接近的 code 的code index （相当于就是把连续的、无限的化为离散的有限的） 由重建误差、特征误差、感知损失、判别损失组成 图像的标记需要排列为 1D 序列，常用的方法是光栅扫描、螺旋顺序或 Z 曲线顺序。 Discussion on the weakness of vanilla autoregressive models. Mathematical premise violation. 在 VQVAEs 中，编码器生成的图像特征图 f 是一个包含多个特征向量 的二维矩阵，这些特征向量之间是相互依赖的。在对这些特征进行量化和扁平化后，生成的令牌序列（如 ）保留了二维特征向量之间的双向相关性。这与自回归模型的单向依赖假设相矛盾。 这一点导致了自回归模型在处理图像时的基本假设被破坏，使得模型的预测和生成能力受到限制。 Inability to perform some zero-shot generalization. 自回归模型的单向特性限制了其在需要双向推理的任务中的泛化能力。例如，它无法在给定底部的情况下预测图像的顶部。 Structural degradation 在特征图中，像素（或令牌）之间的空间局部性很重要。例如，令牌 与其四个相邻令牌 和在空间上是相关的。而扁平化过程破坏了这种空间局部性。 Inefficiency 生成图像令牌序列 需要多次自回归步骤。传统自回归模型的计算复杂度为 的自回归步骤和 的计算成本，效率非常低。 2.2 Visual autoregressive modeling via next-scale prediction Reformulation. 量化特征图 输入的特征图 。作者将其量化为 K 个多尺度令牌图， 每个令牌图的分辨率逐步提高，直到最后的 与原始特征图的分辨率相同。 自回归似然 。每个 的生成依赖于其之前的令牌图。 在第 k 步中，所有 中的 个令牌将被并行生成。 在VAR的训练中，使用块状因果注意力掩蔽，确保每个 只能关注其前缀 。 在推理阶段，可以使用 kv-caching ，提高效率。 Multi-scale VQVAE Encoding transformer 根据特征图 f 预测更高的分辨率的令牌图,取 最接近的 code 的 code index 入队列 transformer 根据 最接近的 code 的所有先前的令牌图预测更高的分辨率的令牌图 f 减去已编码的特征，包含更细致的纹理和细节 Multi-scale VQVAE Reconstruction R中 code index 出队列 transformer 根据 最接近的 code 所有先前的令牌图预测更高的分辨率的令牌图 f 每层循环都加上有更高分辨率的 code Discussion. VAR解决了传统自回归模型的一些问题： 数学前提的满足： 通过限制每个 只依赖于其前缀 ，确保了模型的数学假设得到满足。这种设计与人类视觉感知的“粗到细”特点相符。 空间局部性的保留： VAR中没有平坦化操作，同时每个 中的令牌是完全相关的。更好地保留图像的空间结构。 生成复杂度的降低： 生成一个 的图像序列的复杂度降低到了 。这种效率增益来自于每个 中的并行令牌生成。 Tokenization. 开发了一种新的多尺度量化自编码器，该自编码器将图像编码为 K 个多尺度离散令牌图 。 使用与 VQGAN 相似的架构，但对多尺度量化层进行了修改。 在上采样 到 时使用了额外的卷积层来处理信息损失。 3 论文实验评估方法与效果 Power-law scaling laws 扩大自回归 (AR) 大语言模型 (LLM) 会导致测试损失 L 的可预测下降，因此可以根据较小模型的表现预测大模型的性能。 Zero-shot task generalization 4 论文局限性 对VQVAE架构进行改进 改进 VQVAE tokenizer 原文链接：Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction"},{"title":"Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation","date":"2024-10-01T16:00:00.000Z","url":"/posts/Autoregressive-Model-Beats-Diffusion-Llama-for-Scalable-Image-Generation/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"],["Autoregressive Model","/tags/Autoregressive-Model/"],["Transformer","/tags/Transformer/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1 研究背景 基于自回归模型的 LLMs 取得了显著进展，部分研究开始探讨自回归模型在图像生成领域的应用，并引入图像标记化技术将连续图像转换为离散标记，从而实现图像标记的生成。 2 论文提出的新思路、新理论、或新方法 首先，图像被量化为离散标记，并通过光栅扫描顺序将这些标记序列化。然后，基于Transformer的自回归模型逐步预测下一个标记，直到生成完整的图像标记序列。最后，这些标记通过解码器还原为原始图像。 2.1 Image Tokenizer 2.1.1 量化自动编码器架构（Quantized Autoencoder Architecture） 使用与VQGAN相同的三部分结构：编码器、量化器和解码器。 编码器 使用下采样比为 p 的 ConvNet 将图像像素 x 投影到特征图 f 量化器 包含一个codebook ，codebook中有 K 个可学习的向量。在量化过程中，编码器输出的每个特征向量 会被映射到 codebook 中最接近的特征向量 的代码索引 。 解码器 使用下采样比为 p 的 ConvNet 在解码阶段，代码索引 被重新映射回其对应的特征向量 ，解码器将这些特征向量转换回原始图像像素 x 。 codebook 的设计 低维向量：使用较低维度 C 的代码向量，使得每个代码的表示更紧凑。 大 codebook 尺寸：增加 codebook 中可用代码的数量 K ，提高量化的多样性和表达能力。 2.1.2 训练损失函数 量化是将连续值（图像像素）转换为离散值（ codebook 中的索引）的过程。这一过程是非可微分的，因此标准的梯度下降方法无法直接应用于量化步骤。 为了解决这个问题，使用了直通梯度估计器（Straight-Through Gradient Estimator），可以在反向传播时允许梯度流动。 公式为： 是停止梯度操作，它确保在反向传播时，量化的影响不会被直接传播，而是保持输入特征 f 的影响。 codebook 学习是使编码器生成的特征向量 f 与 codebook 中的向量 z 之间的距离尽量接近。 codebook 学习损失 第一项主要确保了 codebook 的离散表示能够逼近编码器生成的连续特征，从而实现高质量的量化效果。 第二项是承诺损失（commitment loss），强制编码器输出接近代码库中的向量。 用于调整两项之间的相对重要性。 图像重建损失 是图像像素上的重建损失 是感知损失，使用 LPIPS 度量，主要衡量图像在高层特征空间中的相似性，而不是直接的像素差异。能更好地捕捉到人眼感知的差异。 是对抗损失，来自一个 PatchGAN 判别器，它与图像分块器同时训练。 是对抗损失的权重。 2.2 Image Generation by Autoregressive Models Llama architecture. 模型架构主要基于 Llama 使用 RMSNorm SwiGLU 激活函数 旋转位置嵌入 在模型的每一层都使用 2D RoPE。 不使用 AdaLN 来保持我们的结构与 LLM 相同。 Class-conditional image generation. 类嵌入是指将每个图像类别（如狗、猫、车等）映射为一个可学习的嵌入向量。这个嵌入向量是从一个嵌入矩阵中索引出来的。并用作预填充令牌嵌入。 从该令牌嵌入开始，模型通过下一个令牌预测方式生成图像令牌序列，并在预定义的最大长度的位置停止。 Text-conditional image generation. 使用 FLAN-T5 XL 作为文本编码器 编码的文本特征由额外的 MLP 投影 MLP 处理后的文本特征会被用作自回归模型中的预填充 token 嵌入。 未来的研究可能会探索一种多模态基础模型,在语言和视觉之间建立一个统一的词汇表。 Classifier-free guidance. 无分类器引导是在扩散模型社区中开发的因其提高视觉质量和文本图像对齐而闻名。 在训练期间，条件被随机丢弃并被空条件嵌入取代。在推理中，对于每个标记，其 logit 由 形成，其中 是条件 logit， 是无条件 logit，s 是无分类器指导的尺度。 2.3 Scale Up 本文的模型架构几乎与 Llama 相同，因此可以无缝地采用 LLM 社区中的优化技术和训练方法。 在这项工作中将模型大小扩展到 3.1B 参数。所有模型均使用 PyTorch 2 实现并在 80GB A100 GPU 上进行训练。对于训练参数低于1.4B的模型，我们直接使用DDP，否则，我们采用 PyTorch FSDP 优化 GPU 内存使用。 2.4 Serving 与训练类似，LLM社区开发的推理技术也可以用来优化本文的模型。作者在本文的图像生成方法上验证了 vLLM（最流行的 LLM 服务框架之一） 的有效性。与基线设置相比，实现了 326% 和 414% 的加速。 3 论文实验评估方法与效果 3.1 Image Tokenizer 代码簿设计的影响 减少 codebook 向量的维度（如从256降至8），会显著提高重构质量和代码簿的使用率。 增大 codebook 的大小（如从4096增加到16384）也能提升整体表现。 Token数量对图像重构质量的影响 增加token数量（例如从16×16到24×24）能够显著改善图像质量。更高的token数量意味着模型能够保留更多图像细节，从而提高重构性能。 与其他图像Token生成器的比较 作者的tokenizer在多个评估数据集（如ImageNet和COCO）的重构性能上优于之前的方法，包括VQGAN、MaskGIT等。 重要的是，该tokenizer能够与基于连续潜空间表示的模型（如SD VAE、SDXL VAE等）相竞争，显示出离散表示不再是图像重构的瓶颈。 3.2 Class-conditional Image Generation 图像令牌的效果 增加图像令牌可以提升图像重建质量，但对图像生成质量的影响不显著。 当模型参数小于 1B 时，使用 256 个令牌（16×16 格式）能带来比 576 个令牌（24×24 格式）更好的图像生成效果。 然而，当令牌数量较少时，生成质量会受到限制。例如对于3B的model，256 令牌的 FID 为 3.06，而 576 令牌则能将 FID 降低到更低的值2.61。 模型大小的效果 不同规模的模型（B, L, XL, XXL, 3B）展示了在模型参数和训练轮次增加时，FID 值逐步降低，模型性能随之提升。但当模型扩展至 3B 后，训练轮次增加带来的进一步提升的效果变得有限。 这可能是由于数据集大小的限制（ImageNet 仅有约 100 万张图像），扩展数据集或增强数据增广可能进一步提升性能。 采样策略的效果 使用 CFG 能显著提升视觉质量，尤其是在 CFG 设置为 2.0 时，能取得最优的 FID。 Top-k 采样的效果显示，较小的 top-k 不利于 FID 和 IS，增加 top-k 会提升 FID 但降低 IS，即在保真度和多样性之间产生权衡。 与其他图像生成模型的比较 比较了该模型与其他模型如GAN、Diffusion Models、Masked Prediction Models等主流模型的性能。在多个评估指标上，模型表现出极具竞争力的性能，尤其是 3B 模型超越了一些著名的扩散模型，如 LDM 和 DiT。 该模型的优势来自于图像标记器的改进设计以及图像生成模型的良好可扩展性。 原文链接：Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation"},{"title":"ANIMATEDIFF: ANIMATE YOUR PERSONALIZEDTEXT-TO-IMAGE DIFFUSION MODELS WITHOUT SPECIFIC TUNING","date":"2024-09-24T16:00:00.000Z","url":"/posts/ANIMATEDIFF-ANIMATE-YOUR-PERSONALIZEDTEXT-TO-IMAGE-DIFFUSION-MODELS-WITHOUTSPECIFIC-TUNING/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["视频生成","/tags/%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90/"],["Transformer","/tags/Transformer/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1. 研究背景、动机 在视频生成领域，通常通过时间结构来扩展预训练的T2I模型，但这些方法通常更新所有参数，修改原始T2I模型的特征空间，因此与个性化 T2I 模型不兼容。 预备知识 Stable Diffusion Low-rank adaptation (LoRA) 在LoRA中，模型的权重W不会直接被更新，而是通过添加两个低秩分解矩阵A和B作为残差来更新。 2. 论文提出的新方法 2.1 ALLEVIATE NEGATIVE EFFECTS FROM TRAINING DATA WITH DOMAIN ADAPTER 域适配器通过适配视频与图像数据的领域差异，防止模型在训练时学习到低质量的视频特征。 视频与图像数据的质量差异 视频数据的收集比图像数据更困难，且质量通常较低。 视频帧可能存在运动模糊、压缩失真和水印等问题。与高质量的图像数据相比，这些问题导致了视频和图像之间存在显著的质量域差距。 是标量，可以在推理时调整为其他值（设置为 0 以完全消除域适配器的影响）。 域适配器通过LoRA（低秩适配）实现，并嵌入到T2I模型的自注意力和交叉注意力层中。这种实现方式允许在训练时只优化域适配器的参数，减少对其他部分的干扰。 2.2 LEARN MOTION PRIORS WITH MOTION MODULE Network Inflation 预训练的 T2I 模型中已经学到了非常高质量的图像生成能力。为了不浪费这些已有的知识，作者希望将这些图像层的知识扩展到视频数据上，但同时保留这些图像层处理静态帧的能力。 将输入修改为5D张量，格式为 ，其中： b是批次维度， f是帧的时间维度（代表时间轴）， h和w是图像的高度和宽度。 时间轴（帧数 f）在进入图像层时会被重塑为批量大小 b，每一帧都会被当作一个独立的静态图像来处理。 处理完图像层后，再把特征映射恢复成5D张量格式。 Module Design 运动模块采用的是 Transformer 架构。这里 Transformer 被调整来处理视频的时间维度（即帧与帧之间的关系）。时间 Transformer 由沿时间轴的多个自注意力块组成，并通过正弦位置编码来编码动画中每一帧的位置。 在输入阶段，空间维度（h,w）被整合到 b 中。然后，这些特征图沿着时间轴 f 分割成一系列的向量序列 每个向量对应一个视频帧。这些向量会被投影并进入多个自注意力块。 其中， 是三个独立的投影。通过这种机制，模型可以让每一帧在生成时参考其他帧的信息，从而捕捉动画中的动态变化。 为了让模型知道每帧的顺序，必须在自注意力操作之前加入正弦位置编码。如果没有这个编码，模型就无法感知帧的先后顺序。 时间 Transformer的输出投影层采用了零初始化，并加入了残差连接。以便运动模块在训练开始时是恒等映射。 2.3 ADAPT TO NEW MOTION PATTERNS WITH MOTIONLORA MotionLoRA 是一种高效的微调方法，能够快速适应新的运动模式（如镜头变焦、平移和滚动等）。 将 LoRA 层添加到运动模块的自注意力层，并针对新的运动模式进行训练。 研究中进行了多种镜头类型的实验，利用规则基础的数据增强获取参考视频。例如，为了获得具有缩放效果的视频，我们通过沿时间轴逐渐缩小（放大）或放大（缩小）视频帧的裁剪区域来增强视频。MotionLoRA 显示出即使只使用20到50个参考视频和大约2000次的训练迭代（大约1到2小时），也能够取得令人满意的结果。此外，所需的存储空间也相对较小，约为30MB。 2.4 ANIMATEDIFF IN PRACTICE 2.4.1 Training 训练目标： 域适配器的训练使用与 LDM 中相同的原始目标 运动模块、 MotionLoRA 作为动画生成器的一部分，使用类似的目标，但经过轻微修改以适应更高维度的视频数据。 具体训练过程 视频数据批次 首先通过SD的预训练自动编码器编码为潜在编码 。 使用定义的前向扩散过程对潜在编码进行加噪 训练目标是 2.4.2 Inference 在推理时，个性化的 T2I 模型将首先进行膨胀，然后注入运动模块以生成一般动画，此外还可以选择性地加入 MotionLoRA 以生成个性化的动画。 在推理过程中，域适配器并非简单地被丢弃，而是可以注入到个性化的 T2I 模型中。用户可以通过调整 来调节域适配器的影响。 最后，通过执行反向扩散过程和解码潜在编码，生成动画帧。 3. 论文实验评估方法与效果 3.1 QUALITATIVE RESULTS AnimateDiff 使用了 Stable Diffusion V1.5 作为基础模型，并在 WebVid10M 数据集上训练了运动模块。为了验证模型的效果，AnimateDiff 在来自 Civitai 社区的多个个性化 T2I 模型上进行了评估。这些个性化模型涵盖了不同领域，是一个全面的基准。 实验展示了 8 个 AnimateDiff 的定性结果，每个样本对应一个不同的个性化 T2I 模型。还展示了与 MotionLoRA 结合的效果，通过组合不同的权重来实现复杂的镜头控制。 研究者将 AnimateDiff 与其他最近的基于视频生成的方法（如 Text2Video-Zero 和 Tune-a-Video）以及商业工具（如 Gen-2 和 Pika Labs）进行了对比，证明了 AnimateDiff 在个性化 T2I 动画生成任务上的优越性。 3.2 Quantitative Comparison 研究者通过用户研究和 CLIP 指标对三个关键方面进行了定量评估 文本对齐 领域相似性 运动流畅度 用户研究：通过生成动画并让参与者对其进行排名，得到平均用户排名（AUR）作为模型性能的衡量指标。文本对齐和领域相似性的评价基于提示词和参考图像。 CLIP 指标：CLIP 指标用于评估动画帧与个性化 T2I 生成的参考图像之间的领域相似性。 3.3 Ablative Study Domain adapter 通过调整域适配器中的比例因子，研究了其对视觉质量的影响。随着比例因子减少，视觉质量提高，且视频数据集中的分布偏差（如 WebVid 中的水印）减少。表明域适配器通过减轻运动模块学习视觉分布间隙而在增强 AnimateDiff 视觉质量方面发挥了成功作用。 As illustrated in Figure 6, as the scaler of the adapter decreases, there is an improvement in overall visual quality, accompanied by a reduction in the visual content distribution learned from the video dataset (the watermark in the case of WebVid (Bain et al., 2021)). These results indicate the successful role of the domain adapter in enhancing the visual quality of AnimateDiff by alleviating the motion module from learning the visual distribution gap. 此处随着比例因子减少，视觉质量提高，是否与下面的 the successful role of the domain adapter in enhancing the visual quality 矛盾？:thinking: Motion module design 比较了时序Transformer和1D时序卷积。结果表明，时间 Transformer 更好地捕捉帧与帧之间的动态变化，而全卷积模块未能有效建模帧间的运动变化。 Efficiency of MotionLoRA MotionLoRA 在参数效率和数据效率上进行了实验。研究表明，即使参数量较小，MotionLoRA 依然能够学到新摄像机运动（如放大或缩小）；然而，当参考视频数量过少（如 N=5）时，模型的运动质量显著下降。 4.4 Controllable Generation 将 AnimateDiff 与 ControlNet（注意力机制，零卷积层） 结合，通过提取的深度图序列控制动画生成，展现了高质量的动画效果，特别是在细节（如头发、面部表情）方面的表现非常出色。 与 DDIM（非马尔可夫） 进行对比。 原文链接：ANIMATEDIFF: ANIMATE YOUR PERSONALIZED TEXT-TO-IMAGE DIFFUSION MODELS WITHOUT SPECIFIC TUNING"},{"title":"Prompt-to-Prompt Image Editing with Cross Attention Control","date":"2024-09-23T16:00:00.000Z","url":"/posts/Prompt-to-Prompt-Image-Editing-with-Cross-Attention-Control/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["Diffusion 可控生成","/tags/Diffusion-%E5%8F%AF%E6%8E%A7%E7%94%9F%E6%88%90/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1. 研究背景、动机、主要贡献 1.1 研究背景 像Imagen、DALL·E 2和Parti等LLI模型，展示了出色的语义生成和组合能力，但它们在图像编辑方面存在控制力不足的问题。即使是对文本提示的轻微修改，生成的图像也可能完全不同。 1.2 存在问题(动机) 1.2.1 现有方案 现有方法通常要求用户手动遮罩要编辑的图像区域，只在遮罩区域进行图像修改，保持其他部分不变。这种方法虽有效，但操作复杂且容易忽略遮罩区域中的重要结构信息，因此不适合更精细的编辑，如修改特定物体的纹理。 1.3 主要贡献 本文提出了一种新的、直观的文本编辑方法，通过“Prompt-to-Prompt”操控，利用预训练的文本条件扩散模型来进行语义图像编辑。 关键思想是深入分析交叉注意力层，探索它们在控制生成图像中的语义作用。交叉注意力层的映射将图像像素与从文本提示中提取的单词进行绑定，作者发现这些映射包含丰富的语义关系，并且对生成图像起关键作用。 2. 论文提出的新方法 2.1 Cross-attention in text-conditioned Diffusion Models 本节主要讲利用交叉注意力机制，结合文本嵌入更新空间特征 基于Imagen文本引导合成模型，主要关注于文本到图像的扩散模型，而保持超分辨率过程不变。图像的构图和几何特征主要在64 × 64分辨率时决定。 步骤 每个扩散步骤 t 通过U形网络预测从噪声图像 和文本嵌入 中生成噪声 。最终生成的图像为 。 噪声图像的深层空间特征 被投影到查询矩阵 ，文本嵌入则被投影到键矩阵 和值矩阵 。 注意力图为 其中 d 是 keys and queries 的潜在投影维度。注意力图的元素 表示第 j 个令牌对第 i 个像素的权重。 最终的交叉注意力输出通过加权平均计算得出，即 ，用于更新空间特征 。 Imagen 模型在每个扩散步骤的噪声预测中使用两种类型的注意力层： 交叉注意力层和混合注意力层 。只干预混合注意力的交叉注意力部分。也就是说，只有最后一个通道（引用文本标记）在混合注意力模块中被修改。 2.2 Controlling the Cross-attention 生成图像的空间布局和几何形状取决于交叉注意力图。 像素更容易被描述它们的单词所吸引 将来自原始提示 的注意力图 注入到修改后的提示 ，来生成编辑图像 ，使其既能依据新的提示进行操作，又能保留原始图像的构图。 整体思路 使用两个提示同时迭代原注意力图和修改后的注意力图。 编辑操作 Word Swap a softer attention constrain 通过在早期步骤使用新提示的注意力图，而在后期步骤逐渐减少其影响，可以在保留原有构图的同时，给予生成图像必要的几何自由度，以适应新提示的内容。 通过为不同的令牌分配不同的注入时间戳和适当的处理方式（如重复或平均），模型能够更精细地控制生成图像的内容和结构。 Adding a New Phrase 例如： 仅对两个提示中的共同标记应用注意注入 使用对齐函数 A，它从目标提示 接收标记索引，并在 中输出相应的标记索引，如果不匹配则输出 None。 如果对齐函数 A 返回“none”，则使用新提示的注意力图 对应的值。 如果 A 返回一个有效索引，则使用原始提示的注意力图 中的相应值。 Attention Re–weighting 如 想调整蓬松的程度，可以调整其权重 原文链接：Prompt-to-Prompt Image Editing with Cross Attention Control"},{"title":"Adding Conditional Control to Text-to-Image Diffusion Models","date":"2024-09-22T16:00:00.000Z","url":"/posts/Adding-Conditional-Control-to-Text-to-Image-Diffusion-Models/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["Diffusion 可控生成","/tags/Diffusion-%E5%8F%AF%E6%8E%A7%E7%94%9F%E6%88%90/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1. 研究背景、动机、主要贡献 1.1 存在问题(动机) 文本到图像模型在对图像的空间组成提供的控制方面受到限制。仅通过文本提示来精确表达复杂的布局、姿势、形状和形式可能很困难。生成与我们的心理想象准确匹配的图像通常需要多次反复试验，包括编辑提示、检查生成的图像，然后重新编辑提示。 而对于让用户提供其所需图像。特定条件的数据量通常远小于用于一般训练的数据量。这种数据不平衡导致过拟合和灾难性遗忘的问题。 1.2 主要贡献 提出了ControlNet，一种神经网络架构，可以通过有效的微调将空间局部化的输入条件添加到预训练的文本到图像扩散模型中。 提出预训练的ControlNets，能够基于多种条件（如Canny边缘、Hough线、用户涂鸦、人类关键点、分割图、形状法线、深度和卡通线条绘制）来控制Stable Diffusion。 通过消融实验和用户研究验证了该方法的有效性，比较了与多种替代架构的性能，并在不同任务中进行了评估。 1.3 相关工作 1.3.1 Finetuning Neural Networks HyperNetwork：最初用于自然语言处理，通过训练一个小的递归神经网络来影响较大网络的权重，之后应用到生成对抗网络（GAN）和图像生成中，并被用来改变Stable Diffusion生成图像的艺术风格。 Adapter方法：在自然语言处理和计算机视觉中广泛应用。它通过嵌入新模块来定制预训练模型，用于增量学习、领域适应以及将预训练的主干模型迁移到不同任务中。 Additive Learning：通过冻结原始模型权重，添加少量新参数避免遗忘，方法包括使用权重掩码、剪枝或硬注意力等。 Side-Tuning：通过添加一个侧支网络并线性混合冻结模型和新网络的输出，学习额外功能，同时避免影响原始模型的表现。 LoRA (低秩适应)：通过低秩矩阵学习参数的偏移，避免灾难性遗忘。该方法基于许多过参数化模型位于低维子空间的观察结果。 零初始化层：ControlNet使用零初始化层连接网络块，确保在训练初期不会添加有害噪声。这种方法在神经网络权重初始化和操控中广泛讨论，其他相关的初始化策略如高斯初始化等也被提及。 1.3.2 Image Diffusion 图像扩散模型 Glide 是一个支持图像生成和编辑的文本引导扩散模型。 Disco Diffusion 使用CLIP引导处理文本提示。 Stable Diffusion 是基于潜在扩散的一个大规模实现。 Imagen 不使用潜在图像，而是通过像素金字塔结构直接扩散。 商业化产品包括 DALL-E2 和 Midjourney。 控制技术 MakeAScene 和 SpaText 利用分割掩码控制图像生成。 GLIGEN 学习注意力层中的新参数，以进行更精确的生成。 Textual Inversion 和 DreamBooth 通过微调模型以少量用户提供的图像进行个性化内容生成。 Prompt-based image editing 提供了通过文本提示编辑图像的实际工具 1.3.3 Image-to-Image Translation 条件 GAN 和 Transformer 可以学习不同图像域之间的映射 Taming Transformer 是一种视觉变换器方法，用于学习图像域之间的映射。 Palette 是从头训练的条件扩散模型，专门用于图像到图像翻译任务。 PITI 是一种基于预训练的条件扩散模型，旨在图像到图像翻译中实现高质量的生成。 2. 论文提出的新方法 2.1 ControlNet 一个神经网络块是一组神经层，如ResNet块、卷积块、多头注意力块等。 ， 代表一个经过训练的神经模块，其中参数 将输入特征图 x 转换为另一个特征图 y 。 为了将 ControlNet 添加到这样的预训练神经块中，会将该原始块的参数 冻结（锁定），同时将该块克隆到具有参数 的可训练副本。可训练副本接收外部的条件向量 c 作为输入。 当这种结构应用于 Stable Diffusion 等大型模型时，锁定的参数保留了用数十亿张图像训练的生产就绪模型，而可训练的副本重用此类大规模预训练模型来建立一个深度、稳健且强大的骨干来处理不同的输入状况。 为了将可训练的副本与冻结的预训练模型连接，ControlNet使用了零卷积层 ， 是一个 1 × 1 卷积层，权重和偏置初始化为零。 ControlNet的输出公式 因为权重和偏置初始化为零，所以在第一个训练步骤中 由于可训练的副本不仅接收输入图像 x ，还复用预训练模型的能力作为强大的骨干网络，能处理各种复杂的条件输入。零卷积的引入确保了在初始训练过程中，随机噪声不会破坏预训练模型的功能。 2.2 ControlNet for Text-to-Image Diffusion Stable Diffusion 架构 Stable Diffusion 基本上是一个 U-Net 结构，包含编码器、一个中间块和跳跃连接的解码器。它的总共有25个模块，其中17个是主模块，8个是上下采样的卷积层。 主模块每个包含4个 ResNet 层和2个 ViTs，每个 ViT 都包含多个自注意力和交叉注意力机制，处理图像和文本的嵌入（text embeddings）。比如“SD Encoder Block A”包括4个ResNet层和2个ViTs，这个块会重复3次。 文本提示通过 CLIP 文本编码器编码，扩散的时间步通过时间编码器编码。 ControlNet 集成到 Stable Diffusion ControlNet 的结构被应用于 U-Net 的每一个编码层。 ControlNet 创建了一个可训练的Stable Diffusion编码块的副本（包括12个编码块和1个Stable Diffusion的中间块）。这些编码块在4种不同的分辨率下操作（64×64、32×32、16×16 和 8×8），每个分辨率包含3个重复的块。 输出会被添加到12个跳跃连接和U-net的1个中间块上。 由于 Stable Diffusion 使用的是典型的U-net架构，因此这种ControlNet结构也可能适用于其他类似的模型。 计算效率 连接 ControlNet 的方式在计算上是高效的，冻结的模型参数不会进行梯度计算，减少了计算量并节省了GPU内存。 ControlNet对条件图像的处理： Stable Diffusion 去噪过程发生在潜在图像空间。它将512×512像素的图像通过VQ-GAN类似的方法预处理为64×64的潜在图像。 为了在 Stable Diffusion 中引入条件控制（如边缘、姿势或深度图），ControlNet 首先要将输入的512×512像素的条件图像转换为64×64的特征空间向量。这通过一个小型网络 实现，该网络由4个卷积层组成（每个卷积层使用4×4的卷积核和2×2的步幅），然后用ReLU激活并逐层增加通道数。 最终，这些条件图像的特征向量 会输入到 ControlNet 中，以实现条件控制。 2.3 Training 特定任务条件 文本提示 随机将 50% 的文本提示 替换为空字符串。这种方法提高了 ControlNet 对输入图像条件的直接识别能力。 在训练过程中，由于零卷积不会给网络增加噪声，因此模型应该始终能够预测高质量的图像。 模型不会逐渐学会如何处理控制条件，而是通常在不到 10K 次优化步骤后突然开始成功地遵循输入条件生成图像。 2.4 Inference 2.4.1 Classifier-free guidance resolution weighting. Stable Diffusion 使用了一种称为分类器无关引导 (CFG) 的技术来生成高质量图像。 分别是模型的最终输出、无条件输出、条件输出和用户指定的权重。 当通过 ControlNet 添加条件图像时，条件图像可以被添加到 , 两者，或者仅添加到 。 如果条件图像被同时添加到 , ，在没有文本提示的情况下，可能会完全消除CFG引导。 如果仅将条件图像添加到 ，则引导效果会非常强烈。 解决方案 将条件图像首先添加到 ，然后根据每个网络块的分辨率大小乘以一个权重 , , 是第 i 个网络块的分辨率大小（例如， ）。 通过降低CFG引导的强度，可以得到更加平衡的结果（如图5d），这种方法被称为CFG分辨率加权。 2.4.2 Composing multiple ControlNets. 当需要将多个条件输入（如Canny边缘检测和姿势信息）同时应用于一个Stable Diffusion实例时，可以直接将多个ControlNet的输出添加到模型中。 在这种情况下，不需要额外的权重调整或线性插值。多个ControlNet的组合可以无缝地直接与Stable Diffusion进行集成，确保不同条件输入的协同工作。 3. 论文实验评估方法与效果 3.1 消融实验 为了模拟真实用户的行为，设置了四种提示语场景： 无提示 不足提示：提供的提示没有充分覆盖条件图像中的对象，例如默认提示“高质量、详细、专业的图像”。 冲突提示：提示内容与条件图像的语义不一致。 完美提示：准确描述生成内容所需的提示，例如“一个漂亮的房子”。 3.2 Quantitative Evaluation User study Comparison to industrial models Stable Diffusion V2 Depth-to-Image (SDv2-D2I)是在大型计算资源上训练的，涉及数千小时的GPU计算和超过1200万的训练图像。 与SDv2-D2I相比，ControlNet只使用了200k的训练样本，在一台单一的NVIDIA RTX 3090Ti上训练了5天。 生成各100张图像，邀请用户判断哪些图像来自SDv2-D2I，哪些来自ControlNet。结果显示用户的平均准确率为0.52±0.17，表明两种方法生成的图像几乎无法区分。 Condition reconstruction and FID score 原文链接：Adding Conditional Control to Text-to-Image Diffusion Models"},{"title":"DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation","date":"2024-09-21T16:00:00.000Z","url":"/posts/DreamBooth-Fine-Tuning-Text-to-Image-Diffusion-Models-for-Subject-Driven-Generation/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["Diffusion 可控生成","/tags/Diffusion-%E5%8F%AF%E6%8E%A7%E7%94%9F%E6%88%90/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1. 研究背景、动机、主要贡献 1.1 存在问题(动机) 现有的文本到图像生成模型可以根据文本提示生成高质量和多样化的图像，但它们无法在不同的场景中一致地再现特定主体。 因为即使使用详细的文本描述，现有模型的输出域表达力有限，生成的图像可能会与给定的参考主体外观不同。 1.2 主要贡献 作者提出了一种“个性化”方法，将主体绑定到模型的输出域中（不是在潜在空间操作，是在像素层面），使其可以根据用户提供的图像生成新图像。 具体来说，用户提供一些主体的图片（大约3-5张），模型通过微调生成包含该主体的图像，且能在不同场景下保持主体的关键特征。 本文的技术是第一个解决主题驱动生成这一新的挑战性问题的技术，允许用户从几张随意拍摄的主题图像中合成该主题在不同背景下的新颖再现，同时保持其特色鲜明。 1.3 相关工作 图像合成 图像合成技术的目标是将给定的主体融入新背景中，使其看起来自然。这通常依赖3D重建技术，但这些方法往往仅适用于刚性物体，且需要大量视角。它们在处理光照、阴影等场景整合问题时表现不足，无法生成全新的场景。 本文提出的方法能够生成在新姿势和场景中的主体图像。 文本驱动的图像编辑和合成 近年来，结合生成对抗网络和图像文本表示的文本驱动图像编辑取得了显著进展。然而，这些方法通常只适用于结构化场景（例如人脸编辑），在处理多样化主体时效果不佳。 最近的扩散模型在多样化数据集上的生成质量超越了GAN，但仍无法一致地保持特定主体在生成图像中的身份。 现有大规模文本到图像生成模型（如Imagen、DALL-E2、Stable Diffusion等）虽然生成效果突出，但难以精细控制生成图像，特别是保持主体的一致性。 可控生成模型： 有多种方法用于控制生成模型，其中一些方法可能适用于以主体为驱动的图像生成任务。例如，Liu等提出的基于扩散的技术可以通过参考图像或文本进行图像变体生成，但这些方法在保持主体特征方面表现不佳。 一些方法使用用户提供的掩码限制修改区域，但这些方法仍无法生成具有一致身份的主体在新场景中的新图像。 在GAN领域，一些方法允许通过微调模型来编辑真实图像，但这些方法通常需要大量图像且仅限于特定领域，如人脸。 2. 论文提出的新方法 2.1 Text-to-Image Diffusion Models 这部分的思路在引入了条件向量这一点和LDM差不多，但是 是真实图像，LDM却没有在像素层面操作。 LDM中是预测噪声， 本文是希望直接优化图片（至少这个公式是这样的）， 条件向量 ， 是文本提示 是真实图像 是控制噪声表和样本质量的项 2.2 Personalization of Text-to-Image Models 使用少量样本数据来微调模型，但这种方式容易导致过拟合或模式崩溃。人们已经对避免这些陷阱的技术进行了研究，但这些研究通常不涉及主体的保留。 此外作者发现大型文本到图像的扩散模型似乎擅长将新信息集成到其领域中，而不会忘记先验信息或过度拟合一小组训练图像 2.2.1 Designing Prompts for Few-Shot Personalization 目标是将一个新的“唯一标识符”与主体绑定，植入模型的“字典”中，使模型可以识别和生成该主体的多样化图像。 为简化操作，模型并不需要每张图片的详细描述，而是使用简单的标签，如 a [identifier] [class noun] [identifier] 是唯一标识符，指代具体的主体； [class noun] 是该主体的粗略类别描述（如猫、狗、手表等）。 类描述符可以由用户提供或通过分类器获得。这种方式利用了模型对类别的先验知识，使其生成符合该类别的多样化图像。如果使用错误的类别描述，可能会增加训练时间并降低生成质量。 2.2.2 Rare-token Identifiers 一些常用的英语单词（如“unique”、“special”）不适合作为标识符，因为模型需要学习将这些词与其原始含义区分开，并重新与主体绑定，这增加了学习难度。 因此，需要选择稀有标记符，即在语言模型和扩散模型中都有较弱先验的标识符。通过随机选取一些字母组合生成稀有标识符并不理想，因为模型往往会将每个字母单独处理，导致这些标识符仍然具有较强的先验知识。 本文提出通过查找模型词汇表中的稀有标记符，通过去标记器将这些标记符转化为对应的文本字符序列，得到实际的标识符。 从词汇表中查找稀有标记符，得到一系列稀有标记符的标记序列 ，该序列可以具有可变长度 k，并且发现相对较短的 k = {1, ..., 3} 序列效果很好 。 是源自标记 的解码文本。 通过 上的去标记器将标记序列解码为字符序列，生成定义唯一标识符 的字符序列。 对于Imagen模型，选择3个或更少Unicode字符的标记，并使用T5-XXL标记器范围内的标记（如5000到10000之间的标记）效果较好。 2.3 Class-specific Prior Preservation Loss 语言漂移 在语言模型中，当模型从大规模文本语料库上预训练后，再进行特定任务的微调时，模型会逐渐失去其对语言的语法和语义理解能力。本文首次发现了类似现象在扩散模型中的存在，模型在对特定主体进行微调时，逐渐忘记了如何生成该类主体（例如，目标主体所属类别的其他对象，如生成特定狗的图像时，可能逐渐失去生成其他狗的能力）。 输出多样性减少 扩散模型本身具备生成多样化图像的能力，但在对少量图像进行微调时，可能会导致模型生成的主体视角、姿势等变得单一，特别是在训练过长时间后，模型往往会“固定”在少数几个训练视角上。 为了缓解上述两个问题，作者提出了一种自生成的类特定先验保持损失。这个方法的核心思想是用模型自己生成的样本来监督模型，以便在小样本微调开始后保留先验信息。 在模型微调之前，利用冻结的预训练扩散模型生成一定数量的属于相同类别的样本 。 ， 。 这些样本被用于保持模型的先验知识，并在微调过程中通过损失函数来监督模型。 损失函数设计 损失函数的第一项是用于去噪的传统扩散模型损失。 损失函数的第二项是先验保持损失，它用自己生成的图像来监督模型，并且 λ 控制该项的相对权重。 该方法不仅提高了模型输出的多样性，还有效解决了语言漂移问题，允许模型进行更多的训练迭代而不会出现过拟合。 通过实验，作者发现约1000次迭代、学习率为1e-5（Imagen）或5e-6（Stable Diffusion）时，能够获得良好的结果。使用3到5张主体图像作为训练数据，每次训练大约生成1000个“a [class noun]”样本，整个训练过程在TPUv4（Imagen）或NVIDIA A100（Stable Diffusion）上仅需5分钟。 3. 论文实验评估方法与效果 Subject Fidelity（主体保真度） 衡量生成图像与输入图像中的主体是否一致，尤其是细节保留。CLIP-I可以用于这个目的，但DINO由于其自监督训练方式，鼓励区分主题或图像的独特特征，更适合用来区分相似主体，效果更好。 Prompt Fidelity（提示保真度）: 评估生成的图像是否准确反映了输入的文本提示。CLIP-T通过比较生成图像与文本提示的CLIP嵌入来计算相似度，从而衡量提示的准确性。 DreamBooth 与 Textual Inversion的对比实验 Ablation Studies Prior Preservation Loss Ablation PRES 用来衡量先验信息保持情况 DIV 衡量的是生成图像的多样性 Class-Prior Ablation 正确的类名对于微调生成模型至关重要，能够引导模型更好地利用类的先验知识，从而生成高质量且一致的图像。 错误类名会导致类先验和主体特征之间的冲突，生成的结果往往失真或不合逻辑。 不使用类名则会使模型缺乏生成的引导，导致生成结果质量低下，训练难以收敛。 关键应用 重新场景化（Recontextualization） 艺术风格呈现（Art Renditions） 新视角生成（Novel View Synthesis） 属性修改（Property Modification） 4. 论文局限性 生成的图像未能准确反映给定的上下文提示 主体的外观会因提示的上下文而发生变化 当提示与主体在训练数据中的原始场景相似时，模型可能会过度拟合原始图像，导致生成的图像缺乏变化。 一些主体比其他主体更容易学习（例如，狗和猫）。对于较为罕见的主体，模型可能无法支持足够多的变体生成。 在某些生成图像中，主体的细节保真度可能会有所下降，甚至出现虚构的主体特征。 原文链接：DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"},{"title":"An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion","date":"2024-09-20T16:00:00.000Z","url":"/posts/An-Image-is-Worth-One-Word/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["Diffusion 可控生成","/tags/Diffusion-%E5%8F%AF%E6%8E%A7%E7%94%9F%E6%88%90/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1. 研究背景、动机、主要贡献 引入新概念到大模型中往往是困难的。因为重新训练模型非常昂贵，而仅用少量示例进行微调通常会导致“灾难性遗忘”——模型忘记了先前学到的知识。尽管有些方法通过冻结模型并训练转换模块来适应新概念，但这些方法依然面临知识遗忘或无法同时访问新旧概念的难题。 解决方案：文本嵌入空间中的新“词” 1.1 相关工作 1.1.1 文本引导的图像合成 文本引导的图像合成最早是在生成对抗网络（GAN）中研究的。通过给定的图像-文本配对数据集，模型会被训练生成相应的图像。现代的方法引入了注意力机制和跨模态对比方法，以提高生成图像与文本描述的匹配度。 近年来，随着大规模自回归模型和扩散模型的出现，生成图像的质量有了显著提升。这些模型的应用领域不仅限于图像生成，还包括图像编辑、视频合成、运动生成等。 然而，这些方法大多依赖用户通过文本准确描述目标图像的能力。而本文的创新点是引入伪词，扩展了生成模型的词汇量，让模型可以通过新的伪词生成个性化的图像，而无需从头训练模型。 1.1.2 GAN反演 GAN反演是指找到一个潜在表示，使得该潜在向量通过GAN生成与目标图像一致的输出。反演方法通常有两类： - 优化方法：直接优化潜在向量，使之通过GAN生成目标图像。 - 编码器方法：使用大规模数据集训练网络，将图像映射到其潜在表示。 本文采用了优化方法，因为它在处理未见过的新概念时更灵活。而编码器方法在泛化时面临更大挑战，可能需要大规模的网络数据来实现相同的自由度。 1.1.3 扩散模型反演 在扩散模型中，反演可以通过给图像添加噪声，然后利用模型去噪来实现。然而，这种方法往往会显著改变图像内容。改进方法包括通过目标图像的低通滤波数据来指导去噪过程，以及使用闭式反演采样过程，这可以提取一个噪声图并生成相应的图像。 在本文的方法中，作者反演的是用户提供的概念，而不是现有图像，将这些概念表示为模型词汇中的伪词，以便在未来生成过程中用于更广泛和直观的编辑。 1.1.4 个性化 个性化模型一直是机器学习的研究目标，尤其是在推荐系统和联合学习中。 最相关的工作是PALAVRA，该方法利用预训练的CLIP模型检索和分割个性化物体。然而，PALAVRA的任务是判别性的，旨在将物体与其他候选对象区分开来。而本文的方法不仅捕捉到了更多细节，还能实现更自然的图像重建和新场景的合成。 1.2 贡献 提出个性化文本到图像生成任务，生成场景以展示用户提供的特定概念。 在生成模型的背景下提出“文本反演”概念，找到新的伪词嵌入来捕捉高级语义和精细视觉细节。 分析了嵌入空间，展示其在失真与可编辑性之间的权衡，并证明法在这方面表现优越。 通过比较，展示了方法在图像生成的视觉保真度和编辑能力上具有优势。 2. 论文提出的新方法 2.1 LDM 基于LDM实现本方法 2.2 Text embeddings 文本嵌入 文本嵌入是将输入的文本（通常是单词或子词）转换成向量表示的过程。典型的文本编码器模型（如BERT）首先对输入的字符串进行处理，将每个单词或子词转换为token（标记），即某个预定义字典中的索引。 每个token对应于一个唯一的嵌入向量，这个嵌入向量通过索引查找的方式获得。这个嵌入向量表示了该单词或子词在语义空间中的位置，编码了其语义信息。 嵌入向量的学习 文本编码器会在训练过程中学习到这些嵌入向量，使模型可以根据这些向量进行计算和推理，进而理解和生成语言。 嵌入空间逆转 作者选择文本嵌入空间作为逆转目标。具体来说，他们通过引入一个占位符字符串 来表示他们想要学习的新概念。 逆转过程：他们干预嵌入过程，将与token化字符串关联的嵌入向量替换为一个新的、通过优化学习的嵌入向量 。 通过这种方式，模型相当于向词汇表中“注入”了一个新概念，并且可以像使用普通单词一样在生成的句子中使用这个新概念。 2.3 Textual Inversion 文本反演的目标是通过优化从一组图像中学习到的新嵌入向量 ，使得模型能够生成新的视觉概念。作者通过一个小的图像集合（通常是 3-5 张图片）进行直接优化，最终得到一个嵌入向量 ，它能捕捉这个概念的视觉特征。 使用一小组（通常 3-5 张）展示目标概念的图像。这些图像不仅展示了目标物体，还包括不同的背景、姿态等变化。这些变化有助于嵌入向量 捕捉概念的多样性和细节。 通过最小化损失函数（LDM loss）来优化嵌入向量 。让生成的图像尽可能接近输入图像。 优化目标： 在这个过程中，模型保持原本的文本编码器 和去噪网络 不变，只优化嵌入向量 。 为了引导模型生成图像，作者随机采样源自 CLIP ImageNet 模板的中性上下文文本作为文本提示。这些模板包括类似于“A photo of ”或“A rendition of ”的句子，将生成任务与图像内容联系起来，从而为目标概念的嵌入向量 提供上下文。 2.4 Implementation details 保留原始超参数 词嵌入使用一个与目标对象相关的单词（例如“雕塑”或“猫”）的粗略描述来初始化。 实验是在 2 个 NVIDIA V100 GPU 上进行的。这种配置允许进行并行处理，提高了计算效率。使用的批量大小为 4。 基础学习率被设置为 0.005。进一步通过 GPU 数量和批量大小来缩放基本学习率，有效率为 0.04。 所有结果是在 5,000 次优化步骤内生成的。 3. 论文实验评估方法与效果 Image variations Text-guided synthesis Style transfer Concept compositions Bias reduction Downstream applications Image curation 4. 论文局限性 可能仍然难以学习精确的形状，而是融入概念的“语义”本质。 优化时间过长，学习一个概念大约需要两个小时。通过训练编码器直接将一组图像映射到其文本嵌入，这些时间可能会缩短。我们的目标是在未来探索这一领域的工作。 另外，在实验部分作者比较的都是先输入一个小的图像集合（通常是 3-5 张图片）进行优化。本文确实是针对这样一个小的图像集合进行过专门的设计，但是对比的其他模型却没有。这样进行最后生成质量的比较是否有失偏颇？是否应该将图像集合扩大后再进行比较？ 原文链接：An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion"},{"title":"Scalable Diffusion Models with Transformers","date":"2024-09-19T16:00:00.000Z","url":"/posts/Scalable-Diffusion-Models-with-Transformers/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"],["Transformer","/tags/Transformer/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1. 研究背景、动机、主要贡献 传统的扩散模型大多采用U-Net作为主干网络，LDM ( High-Resolution Image Synthesis with Latent Diffusion Models ) 也只是通过交叉注意力机制增强其底层 UNet 主干网 ，而本文提出的DiT模型替代了U-Net，是使用Transformer来操作图像的潜在表示。 2. 论文提出的新方法 Preliminaries Diffusion formulation. Classifier-free guidance. 条件扩散模型 反向扩散过程变为： 无分类器引导的目标 无分类器引导的主要目的是提升生成结果，使得模型能够生成与给定类别c相关的样本，同时避免直接依赖外部分类器。通过这种引导方式，采样过程会倾向于生成那些与条件信息（类别标签c）高度匹配的样本。 采样过程中的引导 可以利用的梯度来引导模型朝着更符合类别条件 c 的方向生成图像。扩散模型的输出可以被解释为一个得分函数，通过调整生成图像的梯度，来优化生成样本的类别匹配度。 s&gt;1是引导的强度参数，当 s=1时，恢复为标准的采样过程；当 s&gt;1时，引导力度加强，生成图像更倾向于符合类别条件c。 为了避免对外部分类器的依赖，模型会在训练过程中随机丢弃条件 c ，用一个学到的“空”嵌入符号\"null\" 替代。这种方法通过让模型在有和没有条件的情况下都进行学习，从而在测试时能够灵活地处理带条件和不带条件的情况。 Latent diffusion models. Diffusion Transformer Design Space Diffusion Transformer 是一种基于Transformer的扩散模型架构，DiT 保留了 ViT 的许多最佳实践，保持Transformer的可扩展性，特别是用于图像生成任务。 Patchify. DiT的输入是空间表示z，对于256×256×3的图像，z的形状为32×32×4。每个图像被划分为多个小补丁（patch），每个补丁的特征维度是4。 DiT 第一层是“patchify”，它通过线性嵌入将每个补丁转换为一个维度为d的token序列。这一步骤使得模型可以处理输入图像的局部特征。 在patchify之后，DiT使用标准 ViT 基于频率的位置嵌入（sine-cosine version），为所有输入token添加位置信息。 生成的token数量T由补丁大小超参数p决定。如果将p减半，T将增加四倍，从而使 transformer Gflops至少增加四倍。然而，改变p对模型的参数总数没有实质性影响。 在DiT的设计空间中，考虑了p=2、4、8三种补丁大小。 DiT block design. 在 patchify 之后，输入token会通过一系列变换器块进行处理。除了噪声图像输入外，扩散模型还可能处理其他条件信息，如噪声时间步 t 、类别标签 c 、自然语言等。 针对条件输入，本文提出了四种不同的变换器块设计，每种设计对标准ViT块进行了小但重要的修改 上下文条件（In-context conditioning）：将 t 和 c 的向量嵌入作为两个额外的token直接添加到输入序列中，与图像token没有区别。这种方法不需要修改标准的ViT块（类似于 ViT 中的 cls tokens），并在最终块后移除条件token，几乎不增加 Gflops。 交叉注意力块（Cross-attention block）：将 t 和 c 的嵌入合并为一个长度为2的序列，与图像token序列分开。该transformer块在自注意力层之后增加了一个多头交叉注意力层。这种设计会显著增加Gflops，约15%的计算开销。 自适应层归一化块（Adaptive layer norm (adaLN) block）：将标准层归一化替换为adaLN。该层通过对 t 和 c 的嵌入向量的和进行回归，学习维度缩放和偏移参数。与其他块设计相比，adaLN在增加Gflops方面最小，因此在计算上最有效，也是唯一一个对所有token应用相同功能的机制。 adaLN-Zero块：借鉴ResNet的研究，发现将每个残差块初始化为恒等函数是有益的。该设计在adaLN基础上进行修改，在任何残差连接之前也回归维度缩放参数 。初始化多层感知机以对所有 都输出零向量，这会将完整的 DiT 块初始化为恒等函数。这意味着在训练初期，adaLN-Zero块不会对输入施加任何变换。与标准的adaLN块一样，adaLN-Zero 向模型添加了可以忽略不计的 Gflops。 Model size. 使用的一系列 N 个的DiT块，每个块的隐藏维度大小为 d 。 参考 ViT ，作者采用了标准的变换器配置，联合调整 N 、d 和注意力头，以实现不同规模的模型。 作者定义了四种模型配置：DiT-S、DiT-B、DiT-L和DiT-XL。这些配置覆盖了从0.3到118.6 Gflops 的一系列模型规模，使作者能够评估模型的扩展性能。 Transformer decoder. 在经过最后一个DiT块后，模型需要将图像token序列解码为两个输出：一个是噪声预测，另一个是对角协方差预测。这两个输出的形状与原始的空间输入相同。 为了解码这些token，使用了一个标准的线性解码器，将每个token的表示转化为所需的输出形状。 在解码之前，首先应用最后层归一化，（如果使用 adaLN 则为自适应）。 每个token被线性解码为一个形状为 的张量，其中 p 是补丁的大小，C 是 DiT 空间输入中的通道数。 最后，将解码后的tokens重新排列成原始的空间布局，使得模型的输出可以直接用于后续的任务或与原始图像进行比较，以获得预测的噪声和协方差。 3. 论文实验评估方法与效果 DiT块设计 作者训练了四个DiT-XL/2模型，使用不同的块设计（in-context、cross-attention、adaLN、adaLN-zero） adaLN-Zero的初始化方式（将每个DiT块初始化为恒等函数）显著优于普通的adaLN 缩放模型大小和补丁大小 训练了12个DiT模型，变化模型配置（S、B、L、XL）和补丁大小（8、4、2）。 随着模型规模增加和补丁大小减小，FID显著改善。 增大模型的深度和宽度也有助于改进FID。 DiT Gflops 对于提高性能至关重要 参数数量并不是唯一影响模型质量的因素。即使在保持模型规模不变时,总参数实际上没有改变，但是减少补丁大小会导致Gflops增加，从而提高性能。 较大的 DiT 模型的计算效率更高 可视化缩放 在训练400K步骤后，从每个DiT模型中采样图像，使用相同的起始噪声、采样噪声和类别标签，旨在直观地观察模型规模和token数量对生成图像质量的影响。 随着模型规模和处理token数量的增加，生成图像的质量有明显改善。 DiT-XL/2模型在图像生成任务上，相比现有最先进的扩散模型取得了显著的性能提升。 增加采样计算量（如增加采样步数）并不能弥补模型计算量的不足。即便较小的模型通过增加采样步数来提升图像质量，较大的模型仍然能够在更少的计算开销下生成更高质量的图像。 原文链接：Scalable Diffusion Models with Transformers"},{"title":"High-Resolution Image Synthesis with Latent Diffusion Models","date":"2024-09-18T16:00:00.000Z","url":"/posts/High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1. 研究背景、动机、主要贡献 传统的扩散模型在像素空间操作，导致计算开销巨大，训练和推理都非常耗时。特别是高分辨率图像合成需要大量的GPU资源，限制了模型的广泛应用。 为了降低计算复杂度，作者提出在预训练的自动编码器的潜在空间中进行扩散模型的训练。使LDM能够在保持视觉细节的同时大大减少计算量。 此外，作者引入了交叉注意力机制，使模型能够进行灵活的条件生成任务，如文本生成图像或边界框生成图像。 主要贡献 比以前的工作提供更忠实和详细的重建 计算复杂度降低 高分辨率图像合成 2. 论文提出的新方法 Perceptual Image Compression 感知压缩模型基于之前的工作，由一个基于感知损失和基于patch的对抗性目标相结合训练的自动编码器组成。与依赖像素级损失（如L2或L1）不同，感知损失和对抗性目标可以确保生成的图像在整体结构和局部细节上都与真实图像一致，从而避免模糊，保持高质量的重建效果。 感知损失（perceptual loss）：感知损失通过衡量重建图像与原始图像在高层次特征空间中的差异，来指导模型生成更逼真的图像。它使用预训练的神经网络（如VGG）提取图像的高层次特征，而不是单纯比较像素级的差异。这样可以更好地保持图像的整体视觉质量和语义一致性。 基于patch的对抗性目标（patch-based adversarial objective）：这是通过生成对抗网络（GAN）来进一步提高图像的局部质量。在这种方法中，判别器会评估图像的局部区域（patch）是否真实，从而迫使生成模型生成更加逼真的局部细节。这种局部对抗性目标可以有效防止重建图像中出现模糊或失真的问题。 自动编码器由编码器和解码器两部分组成。编码器负责将输入的高维图像压缩为低维的潜在表示，解码器则从潜在表示中重建出原始图像。 编码器 对高分辨率图像进行下采样，将其压缩为低维的潜在表示 ，然后解码器 根据潜在表示重建图像 。通过不同的下采样因子，在尽可能减少信息丢失的情况下，压缩图像以减少计算复杂度。 作者为避免潜在空间中出现高方差，采用了两种正则化策略：KL正则化和VQ正则化。通过这两种方法，模型能够更稳定地工作，并且可以在保持图像细节的同时实现较轻的压缩。与之前的将潜在空间表示成一维的自回归方法不同，作者保留了潜在空间的二维结构，这意味着潜在表示保留了空间信息（如图像的高度和宽度），而不是将图像压缩成一维序列。从而更好地保存了图像的空间信息，并且生成效果更加逼真、细腻。 KL正则化（KL-reg.）： 这种方法引入了Kullback-Leibler（KL）散度作为正则化手段。KL散度通常用于衡量两个概率分布之间的差异。在这里，它的作用是将学习到的潜在表示 z 的分布拉近到标准正态分布（mean = 0，variance = 1），类似于变分自编码器（VAE）。 通过在潜在空间上引入KL惩罚，模型可以避免潜在表示的方差过大，保持潜在空间的紧凑性。 VQ正则化（VQ-reg.）： VQ正则化基于向量量化（Vector Quantization, VQ）技术，这种技术在VQ-VAE和VQGAN中广泛应用。它通过离散化潜在空间中的连续表示，使得模型更好地捕捉离散的语义信息。 在这里，VQ层被集成到解码器中，通过将潜在表示限制在特定的离散值上来实现正则化。 VQGAN（Vector Quantized Generative Adversarial Network）是基于VQ的生成模型，作者的设计与VQGAN类似，但VQ层直接融入了解码器结构。 Latent Diffusion Models 相比传统的像素空间，这种低维的潜在空间去除了感知不重要的细节，保留了关键的语义信息，使生成模型能够更加专注于生成重要的图像部分。模型采用UNet架构并通过优化重加权的变分下界，能够在保持高效性的同时生成高质量的图像。 重加权变分下界：在这个模型中，生成过程通过优化一个重加权的变分下界来实现。这种重加权机制允许模型将注意力集中在图像中感知上最重要的部分。损失函数的形式如下： 其中，模型在时间 t 上根据潜在表示 进行去噪。 时间条件的UNet：模型的主干是一个时间条件的UNet。UNet是一种适合图像生成的神经网络架构，通过卷积操作捕捉图像的局部特征，并逐步恢复图像细节。在这个模型中，UNet根据输入的噪声和时间步 t 进行去噪。 在训练时，前向过程是固定的。一旦潜在表示 通过编码器 E 得到，模型可以通过单次通过解码器 D 将潜在表示解码回图像。这种高效的推理过程使得生成图像更加快速。 Conditioning Mechanisms 扩散模型可以通过引入条件输入来进行灵活的图像生成。本文的方法通过交叉注意力机制结合不同模态的输入（如文本、语义图等），并使用特定领域编码器 处理这些输入。这种方法使得扩散模型不仅能够生成高质量图像，还能根据输入的条件对生成结果进行精确控制。训练过程中，去噪自编码器和条件编码器是联合优化的，从而确保模型能够同时学习如何去噪和如何从条件中提取信息。这种机制的灵活性使得模型能够适应多种生成任务。 条件生成模型 条件分布 p(z∣y)：扩散模型可以建模条件分布 p(z∣y)。条件生成意味着模型可以通过给定的输入条件来生成特定的输出。例如，给定某个文本描述，模型可以生成与该描述相关的图像。 条件去噪自编码器 ：噪声输入 、时间步 t，还引入条件信息 y。通过条件去噪自编码器，模型能够在去噪过程中利用 y 来指导生成的方向。y 可以是文本、语义地图、或者其他形式的条件输入。 Beyond Class Labels 以往的扩散模型主要通过类别标签（class labels）或者模糊图像变体（blurred variants of the input image）进行条件生成。然而，将扩散模型与其他条件输入形式结合，目前仍然是一个研究较少的领域。 本文提出将扩散模型与交叉注意力机制（cross-attention mechanism）结合，扩展了模型的条件生成能力。 交叉注意力机制 UNet架构中的交叉注意力：通过交叉注意力机制增强其底层 UNet 主干网。具体地，通过交叉注意力层来处理多种输入模态（如语言提示），从而使模型在不同的输入条件下生成相应的图像。 处理条件输入 y 为了处理不同类型的条件输入 y（如文本、图像或语义图），模型引入了一个特定领域编码器 。该编码器将输入 y 投射到一个中间表示 ，表示为维度为 Missing superscript or subscript argumentMd_ 的矩阵。 通过交叉注意力机制，模型能够将条件输入 y 的中间表示用 传递给UNet的中间层。具体的注意力机制计算如下： Q 是对UNet中间表示 进行线性变换后的查询矩阵。 K 和 V 是通过将条件输入 进行线性变换后的键和值矩阵。 注意力机制通过对 Q 和 K 的点积进行加权，从 V 中提取相关信息，从而在生成过程中融入条件信息。 条件扩散模型的训练 损失函数变为： 损失函数衡量模型预测的去噪结果 与真实的噪声 之间的差异。 和 是联合优化的，所以模型不仅学习去噪的过程，还学习如何从条件输入中提取有用信息。 可以由特定领域的专家进行参数化 3. 论文方法的理论分析或实验评估方法与效果 3.1 感知压缩的权衡分析 探讨不同下采样因子 f 对模型生成质量和计算效率的影响，分别在 进行实验。 较小的压缩因子（如 LDM-1）需要更多的训练时间，而较大的压缩因子（如 LDM-32）虽然计算效率高，但生成的图像质量较差。适中的压缩因子（如 LDM-4 到 LDM-16）在计算效率和图像质量之间达到了良好的平衡 。 3.2 无条件图像生成 在 CelebA-HQ、FFHQ、LSUN-Churches 和 ImageNet 数据集上训练了无条件生成模型，图像分辨率为 256x256。 LDM-4 模型在 CelebA-HQ 数据集上达到了新的最优 FID 分数，在生成质量上超越了GAN和VAE等方法 。 3.3 文本条件生成 在 MS-COCO 数据集上测试文本条件生成，使用交叉注意力机制将文本提示与生成图像关联。 LDM 模型在文本条件生成任务中的表现优于其他基于自回归模型的生成方法，如 DALL-E 和 CogView，在 FID 和生成多样性方面具有优势 。 3.4 超分辨率与图像修复 超分辨率：LDM 模型在 ImageNet 数据集上进行了 64x64 到 256x256 的超分辨率生成实验，生成的图像细节更为逼真。 图像修复：在 Places 数据集上进行了图像修复实验（inpainting），特别是对图像中 40-50% 被遮挡区域进行修复。 4. 论文创新点 将潜在扩散模型应用于低维潜在空间 降低了计算复杂度，并且通过感知压缩模型去除不重要的高频细节，从而在保持高生成质量的前提下实现高效的图像合成。 引入交叉注意力机制进行灵活的条件生成 引入领域特定编码器来处理不同类型的输入，并通过交叉注意力将这些条件输入的信息注入到生成过程中。 在潜在空间中进行多模态生成的灵活性 不同于以往依赖自回归或离散潜在空间的生成模型，该方法在潜在扩散模型中保留了图像的二维空间结构。这使得模型可以处理更加丰富的输入信息（如语义图、语言提示等），而不是将图像压缩成一维的离散序列。 原文链接：High-Resolution Image Synthesis with Latent Diffusion Models"},{"title":"DENOISING DIFFUSION IMPLICIT MODELS","date":"2024-09-17T16:00:00.000Z","url":"/posts/DENOISING-DIFFUSION-IMPLICIT-MODELS/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1. 研究背景、动机、主要贡献 在 DDPM 中，生成过程被定义为特定马尔可夫扩散过程的逆过程。 本方法通过一类非马尔可夫扩散过程来推广 DDPM。 这些非马尔可夫过程可以对应于确定性的生成过程，从而产生能够更快地生成高质量样本的隐式模型。 2. 论文提出的新方法 VARIATIONAL INFERENCE FOR NON-MARKOVIAN FORWARD PROCESSES DDPM 论文中： , , 本文（DDIM)： , ,(利用 的不同，不断代换就能得出) ， 根据 ， （) SAMPLING FROM GENERALIZED GENERATIVE PROCESSES DENOISING DIFFUSION IMPLICIT MODELS 当 ，给定 和 ，前向过程变得确定性，t = 1 时除外。因此想求 ,不一定要求出 我们将其命名为去噪扩散隐式模型，因为它是使用 DDPM 目标训练的隐式概率模型（尽管前向过程不再是扩散）。 ACCELERATED GENERATION PROCESSES DDLM是利用马尔可夫过程，所以要依赖前面的状态。但是本方法不依赖前面的状态，就可以加速。 此外，本文是在 Improved Denoising Diffusion Probabilistic Models 基础上实现的加速（引入子序列进行采样） 3. 论文实验评估方法与效果 随着时间步数的增加，样本质量提高。 DDIM（η=0）在较短的步数下就能得到比较好的效果，媲美DDPM（η=1）的生成效果。 DDIM 能够在 20 到 100 个步骤内生成质量与 1000 个步骤模型相当的样本，与原始 DDPM 相比，速度提高了 10 到 50 倍。 原文链接：DENOISING DIFFUSION IMPLICIT MODELS"},{"title":"Improved Denoising Diffusion Probabilistic Models","date":"2024-09-15T16:00:00.000Z","url":"/posts/Improved-Denoising-Diffusion-Probabilistic-Models/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1. 研究背景、动机、主要贡献 DDPM的论文有指出与其他基于似然的模型相比，我们的模型不具有竞争性的对数似然。 而本文主要做的两件事情就是提高对数似然和提高采样速度。 本文还发现，DDPM 可以与 GAN 的样本质量相匹配，同时根据召回率衡量，可以实现更好的模式覆盖率。 本文还研究了 DDPM 如何随着可用训练计算量的变化而扩展，并发现更多的训练计算会带来更好的样本质量和对数似然。 2. 论文提出的新思路或新方法 2.1 提高对数似然 2.1.1 Learning （将方差参数化为 和 之间的插值） 在DDPM中，相对于方差，更多关注的是分布的均值。且利用均值相等作为条件，来推导出噪音的权重。 本文从 DDPM 中方差的设置入手（ ，将 固定为 与将其固定为 的样本质量大致相同）。但也发现，随着增加扩散步骤的数量， 和 在更多的扩散过程中保持彼此接近。得出在扩散步骤较大时，模型均值更能决定分布。 固定方差似乎是一个合理的选择，但它没有提到对数似然。事实上，显示扩散过程的前几个步骤对变分下界的贡献最大。因此，可以通过使用更好的 选择来提高对数似然。 本文是将方差参数化为 和 之间的插值。模型输出一个向量 v，每个维度包含一个分量。（不对 v 做约束，但是实践中模型却并没有预测插值范围之外的方差） 2.1.2 Improving the Noise Schedule （重新构建噪声表，使 随着扩散步骤增加，下降更平缓，降低信息破坏的速度） Ho 等人使用的线性噪声表对于高分辨率图像效果很好，但对于分辨率 64 × 64 和 32 × 32 的图像来说效果不佳。 线性计划最后四分之一的潜伏几乎纯粹是噪声，而余弦计划增加噪声的速度更慢 当跳过高达20%的反向扩散过程时，用线性时间表训练的模型不会变得更糟（通过FID测量）。（就存在较多的无效时间） 因此，重新构建噪声表 将 限制为不大于 0.999，以防止在扩散过程结束时接近 t = T 时出现奇点。 余弦时间表设计为在过程中间有一个 的线性下降，同时在 t = 0 和 t = T 的极值附近变化很小，以防止噪声水平突然变化。 使用较小的偏移量 s 来防止 在 t = 0 附近太小，因为在过程开始时存在少量噪声会使网络难以足够准确地进行预测。选择 s= 0.008 使得 略小于像素箱大小 1/127.5。 选择使用 ，因为它是一个常见的数学函数，具有我们正在寻找的形状。 图 5 可以看到 Ho 等人的线性时间表下降到零的速度要快得多，信息破坏的速度比必要的要快。 2.1.3 Reducing Gradient Noise （重要性采样） ，因此希望通过直接优化 而不是通过优化 来实现最佳对数似然。然而， 在实践中实际上很难优化(至少在多样化的 ImageNet 64 × 64 数据集上是这样) 的梯度比 的梯度大很多 注意到 的不同项具有区别很大的不同大小，假设均匀采样 t 会在 目标中产生不必要的噪声。为了解决这个问题，我们采用重要性采样： 由于 事先未知，并且在整个训练过程中可能会发生变化，因此维护每个损失项的前 10 个值的历史记录，并在训练期间动态更新。 2.2 Improving Sampling Speed 所有的模型都经过 4000 个扩散步骤的训练，在现代 GPU 上生成单个样本需要几分钟的时间，但本文的方法使得我们可以在几秒钟而不是几分钟内从我们的模型中进行采样。 对于使用 T 个扩散步骤训练的模型，我们通常会使用与训练期间使用的相同的 t 值序列 (1, 2, ..., T ) 进行采样。本文使用 t 值的任意子序列 S 进行采样。给定训练噪声表 ，对于给定的序列 S，得到采样噪声表 ，相应的采样方差为 为了将采样步骤数从 T 减少到 K，使用 1 到 T（含）之间的 K 个均匀间隔的实数，然后将每个结果数字舍入到最接近的整数。 使用此模型，100 个采样步骤足以为完全训练的模型实现接近最佳的 FID 2.3 Scaling Model Size 为了衡量性能如何随着训练计算而扩展，使用 Lhybrid 目标在 ImageNet 64 × 64 上训练四个不同的模型。 为了改变模型容量，我们在所有层上应用深度乘数，使得第一层具有 64、96、128 或 192 个通道。(之前的实验在第一层使用了 128 个通道。) 由于每层的深度都会影响初始权重的规模，因此我们将每个模型的 Adam 学习率缩放为 ，使得 128 通道模型的学习率为 0.0001 (正如其他实验一样）。 原文链接：Improved Denoising Diffusion Probabilistic Models"},{"title":"Denoising Diffusion Probabilistic Models","date":"2024-09-14T16:00:00.000Z","url":"/posts/Denoising-Diffusion-Probabilistic-Models/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["Diffusion Model","/tags/Diffusion-Model/"],["图像生成","/tags/%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90/"]],"categories":[["Machine Learning","/categories/Machine-Learning/"],["Computer Vision","/categories/Machine-Learning/Computer-Vision/"]],"content":"1. 扩散模型和去噪自动编码器 方法原理 方法原理大概就是对于 Noise Predicter 输入一张有噪音的图、当前的步骤编号（和文字提示），然后 Noise Predicter 生成预测的噪音，再用原来的图片减去噪音，得到更完整的图片。不断迭代。 而 Noise Predicter 的训练资料就是对于完整的图片，不断加入噪音 文字到图片主要就是文字先经过一个好的 Text Encoder ，生成向量，在加入噪音，生成模型就生成中间产物，中间产物图片压缩版本经过Decoder生成最终图片 公式推导 找噪音 实际上并不是一步一步地去噪音的，每一个循环都是在尽量找出对应的噪音，希望能够最大地去噪。 去噪 首先是尝试找到 值，使模型生成的network生成 概率最大 可能无法找到 的最大值，所以会希望它的下界最大。 最终可以化简为（要使下面的式子最小） 要使 最小，希望两个分布的mean最接近。 根据 ，替换 ,得到 需要在 的条件下预测为 。 实际上就是需要预测 2. 实验评估方法与效果 为所有实验设置 T = 1000，将前向过程方差设置为从 线性增加到 的常数。这些常数相对于缩放至 [−1, 1] 的数据较小，确保反向和正向过程具有大致相同的函数形式，同时保持 处的信噪比尽可能小 。为了表示相反的过程，使用类似于未屏蔽的 PixelCNN++ 的 U-Net 主干网，并始终进行group normalization。参数是跨时间共享的，这是使用 Transformer 正弦位置嵌入指定给网络的。在 16 × 16 特征图分辨率下使用自注意力。 原文链接：Denoising Diffusion Probabilistic Models"},{"title":"OPERA：通过过度信任惩罚和回顾分配减轻多模态大语言模型中的幻觉","date":"2024-08-15T16:00:00.000Z","url":"/posts/OPERA-Alleviating-Hallucination-in-Multi-Modal-Large-Language-Models/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["大模型安全-幻觉","/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8-%E5%B9%BB%E8%A7%89/"]],"categories":[["大模型安全","/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/"],["幻觉","/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/%E5%B9%BB%E8%A7%89/"]],"content":"1. 研究背景、动机、主要贡献(Why)1.1 研究背景最近多模态大语言模型的发展使基础模型能够让用户使用图像作为输入进行交互。MLLM 的能力使其能够胜任各种视觉任务。 MLLM 也面临着“幻觉”问题。例如，产生不相关或无意义的响应，识别图像中不存在的颜色、数量和位置方面不准确的对象。这一缺陷给 MLLM 成为值得信赖的助手的实际应用带来了巨大的风险。例如，在模型辅助自动驾驶场景中，这种对道路场景图像的误解可能会导致系统的错误判断并导致严重的交通事故。 1.2 存在问题(动机)1.2.1 现有方案缺点 方法会产生大量的额外成本，包括用于训练的额外指令数据的注释预算、外部知识或模型的集成 1.3 主要贡献 无需引入任何外部数据、知识或额外的培训。 我们揭示了幻觉和过度信任模式的出现，并提出了一种配备回顾重新分配策略的基于惩罚的解码方法。 包括GPT 评估在内的广泛评估证明了OPERA 的卓越性能，它几乎可以作为缓解幻觉的免费午餐。 2. 论文提出的新方法(What)2.1 制定 MLLM 的生成过程2.1.1 输入构造MLLM 的输入包含 图像 MLLM 通常使用视觉编码器从原始图像中提取视觉标记 将视觉标记表示为 。这里N是视觉标记的长度，在大多数情况下它是固定的数字 并使用跨模态映射模块将它们映射到 LLM 的输入空间。 文本。 输入文本使用分词器进行分词 将其表示为。 图像和文本标记连接起来作为最终的输入序列，我们将其表示为， T = N + M 。 2.1.2 模型前向传播 MLLM 使用因果注意掩码以自回归方式进行训练，每个标记根据先前的标记预测其下一个标记。 Causal Mask 主要用于限定模型的可视范围，防止模型看到未来的数据。 h 是 MLLM 最后一层的输出隐藏状态，包含了模型对输入序列的编码和理解 e.g. 假设有一个 MLLM 模型，输入一个句子 \"The weather is nice today.\"，模型经过处理后会有一个隐藏状态 “h”，这个隐藏状态包含了模型对整个句子的编码表示。 假设 “h” 是一个包含 512 维度的向量，其中每个维度可能对应于句子中的不同语义特征。 MLLM 使用词汇头 来投影隐藏状态 h 并获取下一个标记预测的 logits（或概率） e.g. 词汇头，通常是指用于将隐藏状态映射为词汇表中每个单词的概率分布的神经网络层。这个层通常是一个全连接层，其输出是一个向量，每个元素对应于词汇表中一个单词的概率。这个输出向量可以通过 softmax 函数转换为概率分布，用于生成下一个可能的标记。 假设我们有一个掩码语言模型（MLLM），输入是一个经过编码后的序列的隐藏状态 “h”，我们想要预测下一个单词。 假设我们的词汇表中有以下单词：[“speak”, “I”, “Chinese”, “can”]。 现在，我们将隐藏状态 “h” 经过词汇头的处理，得到一个包含四个元素的向量，分别对应于词汇表中的每个单词。这个向量可以表示为 [0.1, 0.6, 0.2, 0.1]。 通过 softmax 函数，我们可以将这个向量转换为概率分布。经过 softmax 处理后，我们得到的概率分布如下所示： “speak”: “I”: “Chinese”: “can”: 2.1.3 解码OPERA 基于 Beam Search这是一种基于累积分数的解码策略。简而言之，对于给定的束大小，束搜索保留候选序列，其中每个候选序列都是带有波束分数的解码序列。当解码 token 时，每个候选假设将根据 logits 中的 Top-概率选择 候选 token。最后，解码过程将输出假设赢得最佳波束分数。 e.g. 2.2 过度信任惩罚2.2.1 前置背景在浅层中，标签词从演示中收集信息以形成语义表示以进行更深入的处理，而深层则从标签词中提取并利用这些信息来制定最终预测。 可视化自注意力图。 非对角线元素表示模型在生成当前输出标记时对其他输入位置的关注程度。 柱状注意力模式通常表现在缺乏大量信息的标记上，例如句号或引号。 柱状注意力模式的令牌通常拥有有限的信息，但却对所有后续令牌的预测产生显着影响。 (a) 聚合模式与最近的“锚定令牌”观察结果一致。 (b)、(c) 显示当上下文中出现更多锚标记时，5,000 张随机选择的 MSCOCO 图像上的 CHAIR 分数（更多幻觉）不断增加。 知识聚合后面的内容大部分都带有推理或者幻觉 2.2.2 具体方法幻觉和知识聚合模式之间存在高概率的共存。然而，这种模式具有显着的滞后性，即，当对应的令牌被解码时，不能立即观察到模式，而是在后续的几个令牌被解码之后，幻觉可能已经发生。为了应对滞后现象，我们提出了“过度信任惩罚”。 当前生成的序列 下一个标志词预测的因果自注意力权重causual self-attention weights 考虑在局部窗口中收集所有先前的自注意力权重来表征知识模式，即局部窗口注意力定义为 其中 k 表示我们在注意力图上裁剪的局部窗口的大小，表示第 j 个标记分配给第 i 个标记的注意力权重 预处理，用零填充矩阵的上三角形并放大注意力值其中为零，σ 是可配置的比例因子。 对注意力矩阵的下三角进行列乘法，并获得列分数向量。直观上，分数越大表示相应位置存在的模式越强。因此，我们选择列向得分向量的最大值作为知识聚合模式的特征。 2.3 回顾-分配策略通常，惩罚项能够惩罚具有知识聚合模式的候选者，并鼓励其他候选者被预测。但也有少数情况是，所有候选者都受到惩罚，而幻觉已经出现。 这个案例促使我们重新思考这种聚合模式的起源：它是由前几个后续标记过度信任摘要标记引起的，而惩罚未能纠正它们。因此，一个直观而激进的想法是，如果我们能够排除导致幻觉的标记并在摘要标记之后重新选择正确的前几个标记，则该模式将大大削弱。 回顾分配策略。 当解码过程遇到知识聚合模式并且幻觉不可避免时，它回滚到摘要令牌并选择除了之前选择的候选者之外的其他候选者用于下一个令牌预测。根据经验，解码回顾的条件被设计为对应于几个连续标记的列分数中最大值的位置重叠，其中我们手动将阈值计数设置为r。与不同模型之间变化的最大值不同，位置计数是一个更加稳健和通用的决策指标。 𝟙 手动指定回滚位置 s 必须是单调不递减的。另外，我们配置了回滚的最大时间 3. 论文方法的理论分析或实验评估方法与效果（How） 不同方法、不同模型，在图像级别和句子级别的幻觉表现 对 MSCOCO 数据集（Microsoft COCO，通过收集包含自然环境中常见物体的复杂日常场景的图像来实现的） 进行 CHAIR 评估，该数据集包含超过 300,000 张图像和 80 个带注释的对象。具体来说，我们在验证集中随机选择 500 张图像，并查询不同的 MLLM 模型，并提示“请详细描述该图像”。 不同方法、不同模型，在不同方面的表现（GPT辅助幻觉评估） OPERA确实帮助模型部分克服了由于其偏见或过度信任问题而导致的幻觉问题。我们还注意到，OPERA 以某种方式稍微减少了 MLLM 输出序列的长度，这可能是由于那些额外的幻觉内容的减少所致。 Beam Search和OPERA、不同模型，在正确性和详细性的表现（GPT辅助幻觉评估） 不同方法、不同模型，在随机、流行和对抗性的表现 不同方法，在 LLaVA-1.5 7B 模型上，生成文本质量（语法、流畅度和自然度）的表现（GPT辅助幻觉评估） 不同方法，在流行的 MLLM 基准上的性能的表现 4. 论文优缺点、局限性、借鉴性优点： 之前接触的一些解决幻觉的方法，很多都是通过给模型对应的指令，让其自动调整，而本文却从更为底层的入手 从更为根本的角度揭示了幻觉的出现原因（过度信任） 验证方面做的很全面。 缺点： 图像标注 改进： 假设序列 已经在摘要标记 处呈现了知识聚合模式，我们打算将解码过程回滚到序列手动指定回滚位置 s 必须是单调不递减的 可以稍微向前回滚一点。比如s-k "},{"title":"SAC3：通过语义感知交叉检查一致性在黑盒语言模型中进行可靠的幻觉检测","date":"2024-07-25T16:00:00.000Z","url":"/posts/SAC3-Reliable-Hallucination-Detection/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["大模型安全-幻觉","/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8-%E5%B9%BB%E8%A7%89/"]],"categories":[["大模型安全","/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/"],["幻觉","/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/%E5%B9%BB%E8%A7%89/"]],"content":"1. 研究背景、动机(Why) 1.1 研究背景 LM 经常表现出一种倾向，即产生极其自信但错误的断言，通常被称为幻觉。这种现象严重阻碍了它们在事实准确性至关重要的领域的适用性。 1.2 存在问题(动机) 不确定性的指标在有限 API 访问的商业黑盒 LM中不可获取：幻觉可以通过捕捉不确定性的指标来检测输出序列。然而，这些指标需要访问令牌级别的日志概率，而这在 ChatGPT 或 Bard 等仅提供有限 API 访问的商业黑盒 LM 中不可用。 1.2.1 现有方案 基于采样的方法，通过建立置信度和自我一致性之间的联系来近似不确定性估计。 缺点：自我一致性并不一定能保证事实答案 依赖外部资源，比如从外部数据库检索知识 2. 论文提出的新思路、新理论、或新方法(What) 2.1 第一阶段：通过语义等效扰动进行问题级交叉检查 通过生成保留语义等价的替代输入来重新表述输入查询，即语义上等效的输入扰动。 1. 根据查询输入 ,通过提示 “For the question [QUERIED QUESTION], provide k semantically equivalent questions” （“对于问题 [QUERIED QUESTION]，提供 k 个语义等效的问题”） 2. 生成质量过滤。进一步仔细检查生成的输入 和查询的输入之间的语义等价性。 Are the following two inputs semantically equivalent? [QUERIED INPUT] [GENERATED INPUT]” (“以下两个输入在语义上是等价的吗？ [查询的输入] [生成的输入]”)，过滤掉与原始输入不具有相同语义的输入。 2.2 第二阶段：使用附加验证器 LM 进行模型级交叉检查 让 表示来自基于给定查询 的目标 LM 的原始响应。 检测 是​​否出现幻觉。引入了一个额外的验证器 LM，表示为 ，用于模型级交叉检查 两个语言模型 、 分别回答第一阶段生成的 k 个问题的回答定义为 从目标LM的回答中，抽取个样本 从验证LM的回答中，抽取个样本 问题级交叉检查 对于 目标LM生成 个样本响应序列 验证LM生成 个样本响应序列 结合自检和交叉检查中抽取的所有样本。收集总样本集 2.3 第三阶段：一致性分数计算 QA 对的语义感知一致性检查 同一问题的表述方式不同，答案（例如“否”和“是”）在词汇上可能不等效。但 QA 对作为一个整体在语义上可能是等效的 自检一致性分数 表示以两个 QA 对作为输入的语义等价检查运算符 。如果两个 QA 对在语义上等效，则运算符 C 返回“Yes”，否则返回“No”。 利用提示来使用 LM 实现检查运算符：“以下两个问答 (QA) 对在语义上是否等效？[QA 对 1] [QA 对 2] ” 将最佳猜测映射到数值语义等效分数：{“Yes”→ 0.0，“No”→ 1.0} 用 来表示原始 QA 对，目标LM 的自检分数可计算为其中 &gt;[!question] &gt;在此处是否有必要比较QA对？还是只比较回答就可以？ 问题级一致性分数 模型级一致性分数 模型级交叉检查一致性得分 跨模型跨问题一致性得分 最终得分 λ 是验证者 LM 的权重因子。除非另有说明，在本实验中默认使用 λ = 1 &gt;[!question]+ &gt;是否需要整体除以（1+λ)，或者前者系数为（1-λ）？ 每个组件并行计算 将最终得分与预设阈值进行比较来做出检测预测 3. 论文方法的理论分析或实验评估方法与效果（How） 3.1 分类QA任务中的效果 50% 幻觉样本和 50% 事实样本情况下，在分类QA任务上比较 SC2 和 SAC3-Q 的表现 100% 幻觉样本、预设阈值 0.5情况下，在分类QA任务上比较 SC2 、SAC3-Q 、、SAC3-all的表现 阈值对检测精度的影响 对于 SC2，很大一部分幻觉样本收到了高度一致的预测 受益于语义等效的问题扰动，SAC3-Q 的分数更加分散在不一致的区域中 3.2 开放域生成QA任务中的效果 AUROC 关于开放域生成 QA 任务 信任差异可以通过在验证者 LM 生成的一致性分数中引入权重 λ 来体现。 例如，如果目标是检测特定领域中的幻觉，并且验证器 LM 是为此领域开发的特定领域模型，我们可以为其分数分配较大的权重（例如，λ &gt; 1.0）。一般情况下，验证者LM是小型开源模型，我们可以应用较小的权重值（例如，λ &lt; 1.0）来抵消验证者LM对最终得分的影响。 验证者 LM 权重对 AUROC 的影响： 不同 LLM（GPT-3.5、GPT-4 和 PaLM 2）在分类和生成 QA 任务上的准确性。 数据集方面：在分类 QA 和生成 QA上评估幻觉检测方法，每个类别包含两个数据集。 分类QA 素数：该数据集包含 500 个问题，询问 1,000 到 20,000 之间随机选择的素数的素性，其中事实答案始终为“是”。合成的幻觉答案是“不，这不是素数”。 参议员搜索：数据集由 500 个问题组成，遵循以下模板：“是否曾经有一位美国参议员代表 [美国州名] 州，其母校是 [美国大学名称]？”。事实的答案总是“不”。我们还会产生幻觉答案：“是的，有一位美国参议员代表[美国州名]州，他的母校是[美国大学名]。” 生成 QA （手动注释答案的真实性） HotpotQA-halu：利用一个多跳推理的数据集，构建含250 个带有手动注释的非事实和事实示例的数据集 NQ-open-halu：关于自然问题的含 250 个带有手动注释的非事实和事实示例的数据集 实验设置 评估 模型 目标 LM： OpenAI 的 gpt-3.5-turbo 验证器 LM： (1）Falcon-7b-instruct（Almazrouei 等人，2023）：由 TII 构建的开源因果解码器模型，在 RefinedWeb 的 1,500B 代币上进行训练（ Penedo 等人，2023）并使用精选语料库进一步增强； （2）Guanaco-33b：通过 QLoRA（Dettmers 等人，2023）调整 OASST1 数据集上的 LLaMA（Touvron 等人，2023）基本模型的开源指令跟踪模型。 实施细节 在执行语义扰动和一致性检查时，将温度设置为 0.0 以获得确定性的高质量输出。 k = 10 对于基于自检的方法 SC2，将温度设置为 1.0 并生成 ns = 10 个随机样本。 对于 SAC3-Q 和 SAC3-QM ，设置 nq = nqm = 1 以减少计算成本。为了进一步降低推理成本，默认设置 nm = 1 ， 使用幻觉检测精度和 ROC 曲线下面积 (AUROC) 来评估性能。除了估计的幻觉分数之外，我们还显示了目标 LM 的语言概率（Tian et al., 2023）以进行比较。 实验细节 4. 论文优缺点、局限性、借鉴性 优点： SAC3方法不依赖于语言模型的内部结构，适用于黑盒语言模型，在实际应用中更为广泛。 考虑到了输入的一致性，检验QA对整体的一致性，而非答案一致性。 改进： 如何增强语义扰动的多样性？ 比如可以完善提示“使用同义词和反义词”、“句式变换”、“改变问题的风格和语调” 交叉检查所带来的效率问题，如何简化交叉检查？（选择最具代表性和关键性的特征进行交叉检查，避免对所有特征都进行全面比对） 该方法的并行只是各个得分计算可以并行。如何设计提示来同时生成多个语义等价的问题变体，如何进行并行的一致性检查 对于频繁出现的问题或类似问题，使用缓存机制存储已生成的问题和其一致性评分，避免重复计算。 论文提到，当验证模型在某一领域表现更好时，可以给它更大的权重。在实际应用中权重的选择，如何实现自动选择一个较优的权重？ 可以通过自动调整机制来确定最优权重，例如使用网格搜索或贝叶斯优化等方法寻找最佳权重值。 "},{"title":"通过自我反思减轻大语言模型中的幻觉","date":"2024-07-18T16:00:00.000Z","url":"/posts/Towards-Mitigating-LLM-Hallucination-via-Self-Reflection/","tags":[["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["大模型安全-幻觉","/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8-%E5%B9%BB%E8%A7%89/"]],"categories":[["大模型安全","/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/"],["幻觉","/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/%E5%B9%BB%E8%A7%89/"]],"content":"1. 研究背景、动机、主要贡献(Why)1.1 研究背景 医疗问答方面 系统能相应各种问题格式 是/否 多选 提取 生成 对医学查询生成流畅且有意义的响应（预训练语言模型的引入） 大语言模型在生成式问答中展现出前景。 1.2 存在问题(动机) “幻觉”问题，即模型生成听起来合理但不忠实或无意义的信息 在医疗领域 幻觉信息可能会对患者护理产生严重后果 不常见的专业概念使医学 GQA 任务变得复杂 目前对LLMs产生的医学答案中幻觉程度的理解仍不明朗 分组查询注意力 (Grouped Query Attention) 是一种在大型语言模型中的多查询注意力 (MQA) 和多头注意力 (MHA) 之间进行插值的方法它的目标是在保持 MQA 速度的同时实现 MHA 的质量。 1.2.1 现有方案 名称 会议名称 年份 方法 Addressing Semantic Drift in Generative Question Answering with Auxiliary Extraction使用辅助提取解决生成问答中的语义漂移 ACL-IJCNLP 2021 2021 在编码器上添加一个提取任务，以获得答案的基本原理，根据提取的基本原理和原始输入，解码器预计会生成高置信度的答案。 Read before Generate! Faithful Long Form Question Answering with Machine Reading在生成之前阅读！通过机器阅读进行忠实的长篇问答 ACL 2022 首先使用检索器从大型外部知识源中搜索相关信息。然后阅读器和生成模块将多个检索到的文档与问题一起作为输入来生成答案。具体来说，阅读器模块采用机器阅读理解（MRC）模型为每个文档中的每个句子生成证据分数，而生成器采用大型预训练的Seq2Seq语言模型，将句子证据分数融合到其生成过程中。 Seq2Seq(Sequence to Sequence)，即序列到序列模型，就是一种能够根据给定的序列，通过特定的生成方法生成另一个序列的方法，同时这两个序列可以不等长。这种结构又叫Encoder-Decoder模型，即编码-解码模型，其是RNN的一个变种，为了解决RNN要求序列等长的问题。 1.3 主要贡献 对医学 GQA 系统中的幻觉现象进行了全面检查。特别是在五个医学 GQA 数据集中应用五个LLMs。 提出了一种交互式自我反思方法，迭代生成答案，直到达到令人满意的水平。 实验结果展示了LLMs无需对特定数据集进行明确培训即可提供有意义的见解的能力。 2. 幻觉分析2.1 模型 Vicuna 通过在 ShareGPT 的用户共享对话上微调 LLaMA 进行训练 AlpacaLoRA 采用低秩适应（LoRA）来复制斯坦福大学 Alpaca 模型的结果 ChatGPT 使用人类反馈强化学习（RLHF）来解释提示并提供全面的响应 MedAlpaca 建立在 LLaMA 框架之上，并在指令调整格式的医学对话和 QA 文本上进行了微调 Robin-medical 使用 LMFlow 在医疗领域微调的 LLaMA 2.2 数据集 PubMedQA 1k 个专家标记的实例 问题来自研究文章的标题 内容来自摘要 长回答来自摘要结论 简洁的yes/no/maybe答案 MedQuAD 包含来自美国国立卫生研究院网站的 47,457 个 QA 对 MEDIQA2019 将挑战赛中得分3和4的答案视为黄金答案 LiveMedQA2017 MASH-QA 包括来自消费者健康领域的 34k QA 对 2.3 结果与讨论问题分类（本文认为前两个是幻觉问题。） 事实不一致 模型回答问题时未能正确回忆相关知识 查询不一致 既没有回答问题也没有适当地调用相关知识 离题 提供与主题相关的信息但不直接解决问题的答案。 模型没有进一步处理掌握的知识（例如归纳、演绎和逻辑推理） 应对这些挑战需要模型能够回忆事实知识、情境理解和推理能力 微调对医学领域的影响 [!note]+ why MedAplpaca 和 Robin-medical 之间的差异表明，指令学习比非指令调整更适合LLMs。 频率的测量 随机选择通用模型生成的 100 个样本 确定问题的关键词或主题，通常是疾病名称 采用 1950-2019 年之间这些关键词的平均频率。（数据来源是Google Ngram Viewer，将其作为自然世界中文本分布和预训练语料库的代理）对于有问题的回答，其关键词的平均频率低于好的回答。低频可能是产生幻觉的潜在原因 3. 缓解幻觉的方法(What) 提出了一个迭代的自我反思过程，该过程利用LLMs生成和完善响应的能力 方法包括三个循环 事实知识获取循环 知识一致回答循环 问题蕴涵回答循环 3.1 事实知识获取循环 模型基于所提供的问题生成背景知识 使用定制的无参考评分器对生成的知识进行事实性评估 Fs(\\mathbf{k}|D,Q)=\\sum_{t=1}^mlogP(k_t|\\mathbf{k_{"},{"title":"嗨！","date":"2024-07-12T16:00:00.000Z","url":"/posts/hello-world/","categories":[["undefined",""]],"content":"哦说声嗨，知道你一定会来"}]