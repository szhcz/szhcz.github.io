[{"title":"SAC3：通过语义感知交叉检查一致性在黑盒语言模型中进行可靠的幻觉检测","date":"2024-07-25T16:00:00.000Z","url":"/2024/07/26/SAC3%EF%BC%9A%E9%80%9A%E8%BF%87%E8%AF%AD%E4%B9%89%E6%84%9F%E7%9F%A5%E4%BA%A4%E5%8F%89%E6%A3%80%E6%9F%A5%E4%B8%80%E8%87%B4%E6%80%A7%E5%9C%A8%E9%BB%91%E7%9B%92%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E8%BF%9B%E8%A1%8C%E5%8F%AF%E9%9D%A0%E7%9A%84%E5%B9%BB%E8%A7%89%E6%A3%80%E6%B5%8B/","tags":[["幻觉","/tags/%E5%B9%BB%E8%A7%89/"],["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["大模型安全","/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/"]],"categories":[["大模型安全","/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/"],["幻觉","/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/%E5%B9%BB%E8%A7%89/"],["论文阅读","/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"]],"content":"1. 研究背景、动机(Why) 1.1 研究背景 LM 经常表现出一种倾向，即产生极其自信但错误的断言，通常被称为幻觉。这种现象严重阻碍了它们在事实准确性至关重要的领域的适用性。 1.2 存在问题(动机) 不确定性的指标在有限 API 访问的商业黑盒 LM中不可获取：幻觉可以通过捕捉不确定性的指标来检测输出序列。然而，这些指标需要访问令牌级别的日志概率，而这在 ChatGPT 或 Bard 等仅提供有限 API 访问的商业黑盒 LM 中不可用。 1.2.1 现有方案 基于采样的方法，通过建立置信度和自我一致性之间的联系来近似不确定性估计。 缺点：自我一致性并不一定能保证事实答案 依赖外部资源，比如从外部数据库检索知识 2. 论文提出的新思路、新理论、或新方法(What) 2.1 第一阶段：通过语义等效扰动进行问题级交叉检查 通过生成保留语义等价的替代输入来重新表述输入查询，即语义上等效的输入扰动。 1. 根据查询输入 ,通过提示 “For the question [QUERIED QUESTION], provide k semantically equivalent questions” （“对于问题 [QUERIED QUESTION]，提供 k 个语义等效的问题”） 2. 生成质量过滤。进一步仔细检查生成的输入 和查询的输入之间的语义等价性。 Are the following two inputs semantically equivalent? [QUERIED INPUT] [GENERATED INPUT]” (“以下两个输入在语义上是等价的吗？ [查询的输入] [生成的输入]”)，过滤掉与原始输入不具有相同语义的输入。 2.2 第二阶段：使用附加验证器 LM 进行模型级交叉检查 让 表示来自基于给定查询 的目标 LM 的原始响应。 检测 是​​否出现幻觉。引入了一个额外的验证器 LM，表示为 ，用于模型级交叉检查 两个语言模型 、 分别回答第一阶段生成的 k 个问题的回答定义为 从目标LM的回答中，抽取个样本 从验证LM的回答中，抽取个样本 问题级交叉检查 对于 目标LM生成 个样本响应序列 验证LM生成 个样本响应序列 结合自检和交叉检查中抽取的所有样本。收集总样本集 2.3 第三阶段：一致性分数计算 QA 对的语义感知一致性检查 同一问题的表述方式不同，答案（例如“否”和“是”）在词汇上可能不等效。但 QA 对作为一个整体在语义上可能是等效的 自检一致性分数 表示以两个 QA 对作为输入的语义等价检查运算符 。如果两个 QA 对在语义上等效，则运算符 C 返回“Yes”，否则返回“No”。 利用提示来使用 LM 实现检查运算符：“以下两个问答 (QA) 对在语义上是否等效？[QA 对 1] [QA 对 2] ” 将最佳猜测映射到数值语义等效分数：{“Yes”→ 0.0，“No”→ 1.0} 用 来表示原始 QA 对，目标LM 的自检分数可计算为其中 &gt;[!question] &gt;在此处是否有必要比较QA对？还是只比较回答就可以？ 问题级一致性分数 模型级一致性分数 模型级交叉检查一致性得分 跨模型跨问题一致性得分 最终得分 λ 是验证者 LM 的权重因子。除非另有说明，在本实验中默认使用 λ = 1 &gt;[!question]+ &gt;是否需要整体除以（1+λ)，或者前者系数为（1-λ）？ 每个组件并行计算 将最终得分与预设阈值进行比较来做出检测预测 3. 论文方法的理论分析或实验评估方法与效果（How） 3.1 分类QA任务中的效果 50% 幻觉样本和 50% 事实样本情况下，在分类QA任务上比较 SC2 和 SAC3-Q 的表现 100% 幻觉样本、预设阈值 0.5情况下，在分类QA任务上比较 SC2 、SAC3-Q 、、SAC3-all的表现 阈值对检测精度的影响 对于 SC2，很大一部分幻觉样本收到了高度一致的预测 受益于语义等效的问题扰动，SAC3-Q 的分数更加分散在不一致的区域中 3.2 开放域生成QA任务中的效果 AUROC 关于开放域生成 QA 任务 信任差异可以通过在验证者 LM 生成的一致性分数中引入权重 λ 来体现。 例如，如果目标是检测特定领域中的幻觉，并且验证器 LM 是为此领域开发的特定领域模型，我们可以为其分数分配较大的权重（例如，λ &gt; 1.0）。一般情况下，验证者LM是小型开源模型，我们可以应用较小的权重值（例如，λ &lt; 1.0）来抵消验证者LM对最终得分的影响。 验证者 LM 权重对 AUROC 的影响： 不同 LLM（GPT-3.5、GPT-4 和 PaLM 2）在分类和生成 QA 任务上的准确性。 数据集方面：在分类 QA 和生成 QA上评估幻觉检测方法，每个类别包含两个数据集。 分类QA 素数：该数据集包含 500 个问题，询问 1,000 到 20,000 之间随机选择的素数的素性，其中事实答案始终为“是”。合成的幻觉答案是“不，这不是素数”。 参议员搜索：数据集由 500 个问题组成，遵循以下模板：“是否曾经有一位美国参议员代表 [美国州名] 州，其母校是 [美国大学名称]？”。事实的答案总是“不”。我们还会产生幻觉答案：“是的，有一位美国参议员代表[美国州名]州，他的母校是[美国大学名]。” 生成 QA （手动注释答案的真实性） HotpotQA-halu：利用一个多跳推理的数据集，构建含250 个带有手动注释的非事实和事实示例的数据集 NQ-open-halu：关于自然问题的含 250 个带有手动注释的非事实和事实示例的数据集 实验设置 评估 模型 目标 LM： OpenAI 的 gpt-3.5-turbo 验证器 LM： (1）Falcon-7b-instruct（Almazrouei 等人，2023）：由 TII 构建的开源因果解码器模型，在 RefinedWeb 的 1,500B 代币上进行训练（ Penedo 等人，2023）并使用精选语料库进一步增强； （2）Guanaco-33b：通过 QLoRA（Dettmers 等人，2023）调整 OASST1 数据集上的 LLaMA（Touvron 等人，2023）基本模型的开源指令跟踪模型。 实施细节 在执行语义扰动和一致性检查时，将温度设置为 0.0 以获得确定性的高质量输出。 k = 10 对于基于自检的方法 SC2，将温度设置为 1.0 并生成 ns = 10 个随机样本。 对于 SAC3-Q 和 SAC3-QM ，设置 nq = nqm = 1 以减少计算成本。为了进一步降低推理成本，默认设置 nm = 1 ， 使用幻觉检测精度和 ROC 曲线下面积 (AUROC) 来评估性能。除了估计的幻觉分数之外，我们还显示了目标 LM 的语言概率（Tian et al., 2023）以进行比较。 实验细节 4. 论文优缺点、局限性、借鉴性 优点： SAC3方法不依赖于语言模型的内部结构，适用于黑盒语言模型，在实际应用中更为广泛。 考虑到了输入的一致性，检验QA对整体的一致性，而非答案一致性。 改进： 如何增强语义扰动的多样性？ 比如可以完善提示“使用同义词和反义词”、“句式变换”、“改变问题的风格和语调” 交叉检查所带来的效率问题，如何简化交叉检查？（选择最具代表性和关键性的特征进行交叉检查，避免对所有特征都进行全面比对） 该方法的并行只是各个得分计算可以并行。如何设计提示来同时生成多个语义等价的问题变体，如何进行并行的一致性检查 对于频繁出现的问题或类似问题，使用缓存机制存储已生成的问题和其一致性评分，避免重复计算。 论文提到，当验证模型在某一领域表现更好时，可以给它更大的权重。在实际应用中权重的选择，如何实现自动选择一个较优的权重？ 可以通过自动调整机制来确定最优权重，例如使用网格搜索或贝叶斯优化等方法寻找最佳权重值。 "},{"title":"通过自我反思减轻大语言模型中的幻觉","date":"2024-07-18T16:00:00.000Z","url":"/2024/07/19/Towards%20Mitigating%20LLM%20Hallucination%20via%20Self%20Reflection/","tags":[["幻觉","/tags/%E5%B9%BB%E8%A7%89/"],["论文阅读","/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"],["大模型安全","/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/"]],"categories":[["大模型安全","/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/"],["幻觉","/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/%E5%B9%BB%E8%A7%89/"],["论文阅读","/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"]],"content":"1. 研究背景、动机、主要贡献(Why) 1.1 研究背景 医疗问答方面 系统能相应各种问题格式 是/否 多选 提取 生成 对医学查询生成流畅且有意义的响应（预训练语言模型的引入） 大语言模型在生成式问答中展现出前景。 1.2 存在问题(动机) “幻觉”问题，即模型生成听起来合理但不忠实或无意义的信息 在医疗领域 幻觉信息可能会对患者护理产生严重后果 不常见的专业概念使医学 GQA 任务变得复杂 目前对LLMs产生的医学答案中幻觉程度的理解仍不明朗 分组查询注意力 (Grouped Query Attention) 是一种在大型语言模型中的多查询注意力 (MQA) 和多头注意力 (MHA) 之间进行插值的方法 它的目标是在保持 MQA 速度的同时实现 MHA 的质量。 1.2.1 现有方案 名称 会议名称 年份 方法 Addressing Semantic Drift in Generative Question Answering with Auxiliary Extraction使用辅助提取解决生成问答中的语义漂移 ACL-IJCNLP 2021 2021 在编码器上添加一个提取任务，以获得答案的基本原理，根据提取的基本原理和原始输入，解码器预计会生成高置信度的答案。 Read before Generate! Faithful Long Form Question Answering with Machine Reading在生成之前阅读！通过机器阅读进行忠实的长篇问答 ACL 2022 首先使用检索器从大型外部知识源中搜索相关信息。然后阅读器和生成模块将多个检索到的文档与问题一起作为输入来生成答案。具体来说，阅读器模块采用机器阅读理解（MRC）模型为每个文档中的每个句子生成证据分数，而生成器采用大型预训练的Seq2Seq语言模型，将句子证据分数融合到其生成过程中。 Seq2Seq(Sequence to Sequence)，即序列到序列模型，就是一种能够根据给定的序列，通过特定的生成方法生成另一个序列的方法，同时这两个序列可以不等长。这种结构又叫Encoder-Decoder模型，即编码-解码模型，其是RNN的一个变种，为了解决RNN要求序列等长的问题。 1.3 主要贡献 对医学 GQA 系统中的幻觉现象进行了全面检查。特别是在五个医学 GQA 数据集中应用五个LLMs。 提出了一种交互式自我反思方法，迭代生成答案，直到达到令人满意的水平。 实验结果展示了LLMs无需对特定数据集进行明确培训即可提供有意义的见解的能力。 2. 幻觉分析 2.1 模型 Vicuna 通过在 ShareGPT 的用户共享对话上微调 LLaMA 进行训练 AlpacaLoRA 采用低秩适应（LoRA）来复制斯坦福大学 Alpaca 模型的结果 ChatGPT 使用人类反馈强化学习（RLHF）来解释提示并提供全面的响应 MedAlpaca 建立在 LLaMA 框架之上，并在指令调整格式的医学对话和 QA 文本上进行了微调 Robin-medical 使用 LMFlow 在医疗领域微调的 LLaMA 2.2 数据集 PubMedQA 1k 个专家标记的实例 问题来自研究文章的标题 内容来自摘要 长回答来自摘要结论 简洁的yes/no/maybe答案 MedQuAD 包含来自美国国立卫生研究院网站的 47,457 个 QA 对 MEDIQA2019 将挑战赛中得分3和4的答案视为黄金答案 LiveMedQA2017 MASH-QA 包括来自消费者健康领域的 34k QA 对 2.3 结果与讨论 问题分类（本文认为前两个是幻觉问题。） 1. 事实不一致 - 模型回答问题时未能正确回忆相关知识 2. 查询不一致 - 既没有回答问题也没有适当地调用相关知识 3. 离题 - 提供与主题相关的信息但不直接解决问题的答案。 - 模型没有进一步处理掌握的知识（例如归纳、演绎和逻辑推理） 应对这些挑战需要模型能够回忆事实知识、情境理解和推理能力 有问题的答案类别以及相应的代表性示例 每个模型中每一类有问题的答案的发生率 微调对医学领域的影响 &gt;[!note]+ why MedAplpaca 和 Robin-medical 之间的差异表明，指令学习比非指令调整更适合LLMs。 频率的测量 随机选择通用模型生成的 100 个样本 确定问题的关键词或主题，通常是疾病名称 采用 1950-2019 年之间这些关键词的平均频率。（数据来源是Google Ngram Viewer，将其作为自然世界中文本分布和预训练语料库的代理） 对于有问题的回答，其关键词的平均频率低于好的回答。低频可能是产生幻觉的潜在原因 3. 缓解幻觉的方法(What) 提出了一个迭代的自我反思过程，该过程利用LLMs生成和完善响应的能力 方法包括三个循环 事实知识获取循环 知识一致回答循环 问题蕴涵回答循环 交互式自我反思方法概述 3.1 事实知识获取循环 模型基于所提供的问题生成背景知识 使用定制的无参考评分器对生成的知识进行事实性评估 &gt; 待评估的知识为k = {k1, k2, ..., km}。 D 是带注释示例的小样本演示，Q 是给定的问题。 T(·)是提示模板，包括事实性的定义和任务描述：“根据问题，请生成事实性知识。为此，请考虑以下因素：可验证性、客观性和来源的可靠性。请注意，此评估应基于现有的最佳医学知识。：...：...” 如果事实性得分低于评估阶段设定的阈值，指示模型自我反思并精炼知识，并提示：“The factuality score for the knowledge is XXX (less than THRESHOLD_FACTUAL), which means the knowledge is not strongly supported by empirical evidence. Please refine the knowledge to improve its factuality.”(“该知识的事实性得分为 XXX（低于 THRESHOLD_FACTUAL），这意味着该知识没有得到经验证据的有力支持。请完善知识以提高其真实性。”) 这种生成-评分-优化策略会交互重复，直到生成的知识达到令人满意的事实水平。 3.2 知识一致的应答循环 模型会根据所提供的问题和最终知识，使用模板生成答案：“Refer to the knowledge: \"final_knowledge\" and answer the question: XXX with one paragraph.”(“参考知识：“final_knowledge”并用一段话回答问题：XXX。”) 使用CTRLEval 对生成的答案进行一致性评估 如果生成的答案的一致性得分降低了阈值，则提示模型进行内省、自我更正，并用 “The consistency score for the knowledge is XXX (less than THRESHOLD_CONS), which means the alignment and consistency between response and knowledge are low. Please refine the response to improve its consistency.”(“知识的一致性得分为 XXX（低于 THRESHOLD_CONS），这意味着响应和知识之间的一致性和一致性较低。请完善回复以提高其一致性。”) 重复这种生成-评分-优化策略，直到生成的答案达到一致性级别。 3.3 问题-蕴涵回答循环 通过Sentence-BERT嵌入相似性来评估生成的答案的蕴涵 SBERT 在孪生/三元组网络架构中对 BERT 进行了微调。在质量和性能上都有提高。 如果生成的答案不满足满意的蕴含水平，则过程返回到框架的初始阶段，并且重复整个循环，迭代上述阶段。 4. 论文方法的理论分析或实验评估方法与效果（How） 4.1 自动评估 指标： F1(词语级别) ROUGE-L(需要考虑顺序) &gt; 广泛使用的 n-gram 相似性度量通常无法区分幻觉/不正确的答案，并且与人类判断的相关性较弱 进一步引入 Med-NLI（医学自然语言推理）来评估生成的答案与所提供的上下文或参考答案的逻辑一致性/内涵。 SciFive。一种在广泛的生物医学语料库上预训练的 T5 模型。 评估发生的级别 样本级别。评估生成的答案是否细节 (1)、中性 (0) 或与上下文或参考答案相矛盾 (-1)。 句子级别。除了评估生成的答案是否细节 (1)、中性 (0) 或与上下文或参考答案相矛盾 (-1) 外，还采用了CTRLEval 指标的一致性方面。 LCS(X,Y)是X和Y的最长公共子列的长度。m,n分别表示参考摘要（人工摘要）和候选摘要（机器生成的摘要）的长度 样本级别（Sample Level）评估可以提供对模型整体生成能力的宏观了解。 句子级别（Sentence Level）评估通常使用一些句子级别的度量标准来评估生成结果的准确性和流畅性 [!NOTE]+ CTRLEval的一致性： [!note]+ 在CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation一文中，提到了CTRLEval的多个特性（Coherence，Consistency，Attribute Relevance），为何此处只使用一致性？ 4.2 人工评估 指标 除了自动指标外，还使用 Amazon Mechanical Turk 进行人工评估 样本级别 Query-Inconsistent(查询不一致)：答案提供与查询无关的信息或者是无意义且无意义的。 Tangential(离题)：答案提供了与问题相关的信息，但不直接解决问题。 Entailed(蕴含)答案直接解决了问题 句子级别 Fact-Inconsistent(事实不一致)：答案无法通过参考验证或与参考矛盾 Fact-Consistent(事实一致)：答案句子得到给定上下文或网站的支持 Generic(通用)：答案中的句子没有可供判断的陈述 &gt; \"That's an interesting question\",\"There are some advice\" 4.3 结果 自动评估 MedNLI 显着增加 F1 和 Rouge-L 分数的提升有时相对较小 主要是由于这些指标对黄金答案准确性的固有依赖 方法在所有五个数据集上展示了其在不同的语言模型中的有效性 人工评估 降低了 Vicuna 和 ChatGPT 中查询不一致、离题和事实不一致的百分比 4.4 消融实验 改进（Refinement）的影响 省略了评分和改进阶段，只进行生成阶段。 方面描述的影响 省略了提及需要细化的具体方面。通过使用更通用的指令来指示模型进行自我反思：“请完善知识/响应。” 分数的影响 省略了提供准确值。相反，只在说明中描述需要改进的方面：“该知识没有得到经验证据的有力支持。请完善知识以提高其真实性。” “反应和知识之间的一致性和一致性很低。请完善回复以提高其一致性。” 案例研究 证明了方法在解决事实和查询不一致方面的有效性 5. 论文优缺点、局限性、借鉴性 优点： 提出了一种缓解幻觉的方法，且方法在不同数据集和不同大模型之间均验证了有效性。 缺点： “我们的方法在所有五个数据集上展示了其在具有不同参数（包括 7B 和 175B）的语言模型中的有效性”，并未提及175B相关数据。 某些结论推导不严谨 MedAlpaca (Han et al., 2023) is built upon the frameworks of LLaMA and fine-tuned on instruction-tuning formatted medical dialogue and QA texts. Robin-medical (Diao et al., 2023) is fine-tuned LLaMA in the medical domain using LMFlow. MedAplpaca 和 Robin-medical 之间的差异表明，在我们的任务中，指令学习比非指令调整更适合法学硕士。 迭代可能造成性能糟糕(TTL?) "},{"title":"嗨！","date":"2024-07-12T16:00:00.000Z","url":"/2024/07/13/hello-world/","categories":[["undefined",""]],"content":"哦说声嗨，知道你一定会来"}]